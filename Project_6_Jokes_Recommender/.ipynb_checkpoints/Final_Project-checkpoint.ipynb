{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender systems - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Requirements\n",
    "\n",
    "The main goal of the final project in the recommender Systems course is to analyze a massive data set with at least 1 million ratings, preferably using a distributed processing system such as hadoop. In this project I will be analyzing a 1.7 million continuous ratings (-10.00 to +10.00) of 150 jokes rated by 59132 users. Please visit http://eigentaste.berkeley.edu/dataset/ for more information about the data set. The main deliverables of this project are:\n",
    "* Develop a collaborative filtering algorithm in a distributed processing environment using PySpark. The algorithm is implemented on 1.7 million ratings data set, in Hadoop environment. We cannot implement the proposed algorithm on a laptop or a stand-alone machine, since the logic of the algorithm requires a self join with the ratings data set, along with complex processing logic. We will be using Spark 1.6.1 version as our distributed processing algorithm. \n",
    "\n",
    "* Implement SGD (Stochastic Gradient Descent) algorithm on a stan-alone machine, and evaluate the algorithm's performance on the 1.7 million ratings data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering algorithm\n",
    "\n",
    "We will develop a collaborative filtering algorithm based on the cosine similarity between the pairs of jokes that are rated by at least one common user. The implementation of collaborative filtering algorithm specific to Hadoop environment is given below:\n",
    "\n",
    "We have 2 data sets: *jester_ratings.dat* and *jester_items.dat*. The *jester_ratings.dat* data set has the following format (tab separated):\n",
    "\n",
    "_user-id_,  _joke-id_,  _rating_\n",
    "\n",
    "The *jester_items.dat* has the _joke-id_ and the actual joke (HTML text), separated by a \":\"\n",
    "\n",
    "We will be performing the following steps to build a collaborative recommender system using PySpark on Spark environment:\n",
    "\n",
    "1. Normalize the ratings of the items, based on the following logic:\n",
    "\n",
    "   a. Get the mean rating of each joke. \n",
    "\n",
    "   b. Get the mean rating of each user.\n",
    "   \n",
    "   c. Subtract the mean ratings of each joke (say _j_) and the mean rating of user (say _u_) from the actual rating of the joke.\n",
    "\n",
    "2. Using the normalized ratings, compute the cosine similarity between all the pairs of jokes, which are rated by _same users_. The cosine similarity will help us to identify the potential jokes that a user could like based on the jokes which were already liked by the user. To exploit the spark's distributed processing, we will use the following logic to compute the cosine similarity between the pairs of jokes:\n",
    "\n",
    "   a. Read the contents of jester_ratings.dat into an RDD (Resilient Distributed Dataset), with _user-id_ as the key and _(joke-id, rating)_ as values. Let this RDD be _ratings-rdd_. Pratition the RDD for parallel processing.\n",
    "   \n",
    "   b. Get the self join of this RDD (_ratings-rdd_), and filter the rows to eliminate duplicate jokes (details of the filtering process is explained using an example in *Appendix-B*). Let us call this RDD as _joke-pairs-rdd_. The key of this RDD will be _(joke-id-1, joke-id-2)_ and the value will be _(joke-id-1-rating, joke-id-2-rating)_.\n",
    "   \n",
    "   c. Group by the _joke-pairs-rdd_ by its keys, and perform the cosine similarity of the values. Let the resulting RDD be called as _jokes-similarity-rdd_. This RDD will have the cosine similarity between the pairs of jokes. The number of users who rated the jokes pairs is also recorded in the RDD\n",
    "   \n",
    "   d. Write the _jokes-similarity-rdd_ to HDFS\n",
    "   \n",
    "   e. Combine all the components of _jokes-similarity-rdd_ into a single file, and download that file to a local linux directory. The file is finally downloaded to your desired local machine/server location, where your recommendation algorithm will run.\n",
    "\n",
    "3. To make recommendations to a user:\n",
    "\n",
    "   a. Read the cosine similarity file into a Pandas data frame. Call this data frame as _jokes-sim-df_\n",
    "   \n",
    "   b. Get 5 joke-ids which are rated high by the user\n",
    "   \n",
    "   c. For each of the 5 joke-ids, get all the jokes associated with these 5 joke-ids from _jokes-sim-df_. Rank the jokes in the descending order of _(similarity measure, Number of users who rated both the jokes)_\n",
    "   \n",
    "   d. Pick the top 5 jokes from the ordered items, and present them to the user. These items must not be already rated by the user. \n",
    "\n",
    "See *Appendix-C* for the complete implementation of this process using PySpark. *Appendix-B* contains a detailed walk through of the above logic on a small data set. Using the process explained in *Appendix-C*, we are able to obtain a file (jokes_sim.txt) that contains the cosine similarity between each pairs of jokes, along with the number of users who rated both the jokes in the pair. We will use this file to build our collaborative recommender system. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required python packages\n",
    "The following block imports all the required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import time\n",
    "import warnings\n",
    "import itertools    \n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.sparse.linalg import svds\n",
    "from bs4 import BeautifulSoup #To extract text from HTML\n",
    "import io #To process buffer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the similarity measures data\n",
    "\n",
    "The following code will read the jokes_sim.txt file (produced by the PySpark program in *Appendix-C*), cleans the data and creates a data frame with the following format given below. If you do not have Spark environment, you may download the jokes_sim.txt from the git location of this project and build the recommender directly using the cosine similarity scores in the jokes_sim.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the records from the joke similarity scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-1</th>\n",
       "      <th>Joke-2</th>\n",
       "      <th>Sim-score</th>\n",
       "      <th>Total-votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34</td>\n",
       "      <td>124</td>\n",
       "      <td>0.122780</td>\n",
       "      <td>3604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>0.223450</td>\n",
       "      <td>4164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94</td>\n",
       "      <td>144</td>\n",
       "      <td>0.129519</td>\n",
       "      <td>5521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>132</td>\n",
       "      <td>0.255833</td>\n",
       "      <td>6989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "      <td>0.104506</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Joke-1  Joke-2  Sim-score  Total-votes\n",
       "0      34     124   0.122780         3604\n",
       "1      30      60   0.223450         4164\n",
       "2      94     144   0.129519         5521\n",
       "3      42     132   0.255833         6989\n",
       "4      20      54   0.104506          265"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io\n",
    "with open('jokes_sim.txt', 'r') as file :\n",
    "    filedata = file.read()\n",
    "\n",
    "# Replace the target string\n",
    "filedata = filedata.replace('(', '')\n",
    "filedata = filedata.replace(')', '')\n",
    "jokes_sim_df=pd.read_csv(io.BytesIO(filedata), header=None )\n",
    "jokes_sim_df.columns = [\"Joke-1\",\"Joke-2\",\"Sim-score\",\"Total-votes\"]\n",
    "print \"Some of the records from the joke similarity scores:\"\n",
    "display(jokes_sim_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the similarity score should be interpreted as follows:\n",
    "1. The column _Joke-1_ represents a joke ID\n",
    "2. The column _Joke-2_ represents another joke ID. Both _Joke-1_ and _Joke-2_ were rated by at least one user in common\n",
    "3. The column _Sim-score_ represents the cosine similarity between _Joke-1_ and _Joke-2_. The range of this similarity is [-1, 1]. Greater the similarity score, more similar are the jokes.\n",
    "4. The column _Total-votes_ represents the number of people who voted both the jokes present in _Joke-1_ and _Joke-2_ columns in common\n",
    "5. For a joke ID _j_ in _Joke-1_ column, the related joke IDs in _Joke-2_ column are greater than joke ID _j_. For example, for joke ID 42 (Joke-1 = 42), the Joke-2 column must be greater that 42. See the below display of some of the records where Joke-1 = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying some of the records, where Joke-ID = 42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-1</th>\n",
       "      <th>Joke-2</th>\n",
       "      <th>Sim-score</th>\n",
       "      <th>Total-votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5272</th>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>-0.074450</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4449</th>\n",
       "      <td>42</td>\n",
       "      <td>44</td>\n",
       "      <td>0.089601</td>\n",
       "      <td>4084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5112</th>\n",
       "      <td>42</td>\n",
       "      <td>45</td>\n",
       "      <td>0.185390</td>\n",
       "      <td>5274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>42</td>\n",
       "      <td>46</td>\n",
       "      <td>0.252586</td>\n",
       "      <td>6110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4108</th>\n",
       "      <td>42</td>\n",
       "      <td>47</td>\n",
       "      <td>0.166835</td>\n",
       "      <td>6967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8172</th>\n",
       "      <td>42</td>\n",
       "      <td>48</td>\n",
       "      <td>0.178894</td>\n",
       "      <td>6662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8901</th>\n",
       "      <td>42</td>\n",
       "      <td>49</td>\n",
       "      <td>0.195952</td>\n",
       "      <td>6973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8024</th>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "      <td>0.205549</td>\n",
       "      <td>8006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7857</th>\n",
       "      <td>42</td>\n",
       "      <td>51</td>\n",
       "      <td>0.262250</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>42</td>\n",
       "      <td>52</td>\n",
       "      <td>0.163794</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7732</th>\n",
       "      <td>42</td>\n",
       "      <td>53</td>\n",
       "      <td>0.237151</td>\n",
       "      <td>7839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6857</th>\n",
       "      <td>42</td>\n",
       "      <td>54</td>\n",
       "      <td>0.182889</td>\n",
       "      <td>7062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>42</td>\n",
       "      <td>55</td>\n",
       "      <td>0.113013</td>\n",
       "      <td>4609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>42</td>\n",
       "      <td>56</td>\n",
       "      <td>0.254815</td>\n",
       "      <td>6701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>42</td>\n",
       "      <td>57</td>\n",
       "      <td>0.106067</td>\n",
       "      <td>4048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Joke-1  Joke-2  Sim-score  Total-votes\n",
       "5272      42      43  -0.074450          127\n",
       "4449      42      44   0.089601         4084\n",
       "5112      42      45   0.185390         5274\n",
       "4284      42      46   0.252586         6110\n",
       "4108      42      47   0.166835         6967\n",
       "8172      42      48   0.178894         6662\n",
       "8901      42      49   0.195952         6973\n",
       "8024      42      50   0.205549         8006\n",
       "7857      42      51   0.262250          129\n",
       "7000      42      52   0.163794          136\n",
       "7732      42      53   0.237151         7839\n",
       "6857      42      54   0.182889         7062\n",
       "6694      42      55   0.113013         4609\n",
       "896       42      56   0.254815         6701\n",
       "1594      42      57   0.106067         4048"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Displaying some of the records, where Joke-ID = 42\"\n",
    "display(jokes_sim_df[jokes_sim_df[\"Joke-1\"] == 42].sort([\"Joke-2\"]).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the jokes text\n",
    "The following code will process the *jester_items.dat*, extracts the joke IDs and the jokes text, and outputs a data frame called *items_df*. This data frame will be referenced to extract the Jokes text, based on a joke ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the text of joke ID 144, to make sure that we parsed the Jokes text correctly\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>144</td>\n",
       "      <td>A man is driving in the country one evening when his car stalls and won't start. He goes up to a nearby farm house for help, and because it is suppertime he is asked to stay for supper. When he sits down at the table he notices that a pig is sitting at the table with them for supper and that the pig has a wooden leg. As they are eating and chatting, he eventually asks the farmer why the pig is there and why it has a wooden leg. \"Oh,\" says the farmer, \"that is a very special pig. Last month my wife and daughter were in the barn when it caught fire. The pig saw this, ran to the barn, tipped over a pail of water, crawled over the wet floor to reach them and pulled them out of the barn safely. A special pig like that, you just don't eat it all at once!\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Joke-id  \\\n",
       "143      144   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Text  \n",
       "143   A man is driving in the country one evening when his car stalls and won't start. He goes up to a nearby farm house for help, and because it is suppertime he is asked to stay for supper. When he sits down at the table he notices that a pig is sitting at the table with them for supper and that the pig has a wooden leg. As they are eating and chatting, he eventually asks the farmer why the pig is there and why it has a wooden leg. \"Oh,\" says the farmer, \"that is a very special pig. Last month my wife and daughter were in the barn when it caught fire. The pig saw this, ran to the barn, tipped over a pail of water, crawled over the wet floor to reach them and pulled them out of the barn safely. A special pig like that, you just don't eat it all at once!\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"jester_items.dat\", 'r') as file :\n",
    "    filedata = file.read()\n",
    "\n",
    "soup = BeautifulSoup(filedata)\n",
    "text = soup.get_text() \n",
    "\n",
    "#Output the text to a file.\n",
    "with open(\"output.txt\", \"wb\") as outfile:\n",
    "    outfile.write(text)\n",
    "\n",
    "#Define a list \"l\"    \n",
    "l = list()\n",
    "for line in open(r'output.txt'):\n",
    "    #if line != \" \": \n",
    "        l.append(line.strip(\":\\n\"))\n",
    "#print l\n",
    "\n",
    "##Weed out empty elements from the list\n",
    "l = [i for i in l if i != ''] \n",
    "\n",
    "## Define a function that checks if a string has a number\n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "##List object to accumulate the joke IDs\n",
    "id = list()\n",
    "\n",
    "##List object to accumulate the jokes text\n",
    "text = list()\n",
    "\n",
    "##Temporary string object to append jokes text, since\n",
    "##A joke can be split into more than one elements in \"l\"\n",
    "s = str()\n",
    "for i in l:\n",
    "    if RepresentsInt(i):\n",
    "            if len(s) > 0: ##This will ignore the initial condition\n",
    "                text.append(s)\n",
    "                s = str()\n",
    "            id.append(int(i))\n",
    "    else:\n",
    "        s = s + \" \" + str(i)\n",
    "text.append(s)\n",
    "\n",
    "#print len(id)\n",
    "#print len(text)\n",
    "pd.options.display.max_colwidth = 10000\n",
    "items_df = pd.DataFrame(zip(id,text),columns=[\"Joke-id\",\"Text\"])\n",
    "#display(items_df[items_df[\"Joke-id\"] == 8][\"Text\"][0:1000])\n",
    "print \"Displaying the text of joke ID 144, to make sure that we parsed the Jokes text correctly\"\n",
    "display(items_df[items_df[\"Joke-id\"] == 144])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above display confirms that the text has been cleaned properly, since there are no embedded HTML Tags or any unreadable characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the similarity scores\n",
    "\n",
    "**Top 5 similar jokes**\n",
    "\n",
    "Let us get the top 5 most similar jokes. These joke pairs were rated by at least 1000 users in common. We will extract the jokes text from item_df to display the jokes text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 jokes having highest similarity scores and rated by atleast 1000 users\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-1</th>\n",
       "      <th>Joke-2</th>\n",
       "      <th>Sim-score</th>\n",
       "      <th>Total-votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2184</th>\n",
       "      <td>123</td>\n",
       "      <td>140</td>\n",
       "      <td>0.675881</td>\n",
       "      <td>4224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>86</td>\n",
       "      <td>94</td>\n",
       "      <td>0.608029</td>\n",
       "      <td>6061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7150</th>\n",
       "      <td>138</td>\n",
       "      <td>139</td>\n",
       "      <td>0.460189</td>\n",
       "      <td>9707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9510</th>\n",
       "      <td>60</td>\n",
       "      <td>101</td>\n",
       "      <td>0.452867</td>\n",
       "      <td>4078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>58</td>\n",
       "      <td>74</td>\n",
       "      <td>0.452844</td>\n",
       "      <td>3754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Joke-1  Joke-2  Sim-score  Total-votes\n",
       "2184     123     140   0.675881         4224\n",
       "3039      86      94   0.608029         6061\n",
       "7150     138     139   0.460189         9707\n",
       "9510      60     101   0.452867         4078\n",
       "1452      58      74   0.452844         3754"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke IDs:[123, 140]. Similarity score: 0.675880827987, Total votes: 4224\n",
      "Text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>123</td>\n",
       "      <td>When most people claim to be \"killing time\", it's only an expression. When Chuck Norris kills time, the minutes actually cease to exist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>140</td>\n",
       "      <td>Chuck Norris' calendar goes straight from March 31st to April 2nd; no one fools Chuck Norris.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Joke-id  \\\n",
       "122      123   \n",
       "139      140   \n",
       "\n",
       "                                                                                                                                          Text  \n",
       "122   When most people claim to be \"killing time\", it's only an expression. When Chuck Norris kills time, the minutes actually cease to exist.  \n",
       "139                                              Chuck Norris' calendar goes straight from March 31st to April 2nd; no one fools Chuck Norris.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke IDs:[86, 94]. Similarity score: 0.60802866823, Total votes: 6061\n",
      "Text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>86</td>\n",
       "      <td>A neutron walks into a bar and orders a drink. \"How much do I owe you?\" the neutron asks. The bartender replies, \"For you, no charge.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>94</td>\n",
       "      <td>Two atoms are walking down the street when one atom says to the other, \"Oh, my! I've lost an electron!\" The second atom says, \"Are you sure?\" The first replies, \"I'm positive!\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Joke-id  \\\n",
       "85       86   \n",
       "93       94   \n",
       "\n",
       "                                                                                                                                                                                 Text  \n",
       "85                                             A neutron walks into a bar and orders a drink. \"How much do I owe you?\" the neutron asks. The bartender replies, \"For you, no charge.\"  \n",
       "93   Two atoms are walking down the street when one atom says to the other, \"Oh, my! I've lost an electron!\" The second atom says, \"Are you sure?\" The first replies, \"I'm positive!\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke IDs:[138, 139]. Similarity score: 0.460188696353, Total votes: 9707\n",
      "Text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>138</td>\n",
       "      <td>WASHINGTON (Reuters) - A tragic fire on Monday destroyed the personal library of President George W. Bush. Both of his books have been lost. Presidential spokesman Ari Fleischer said the president was devastated, as he had not finished coloring the second one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>139</td>\n",
       "      <td>In a Veteran's Day speech, President Bush vowed, \"We will finish the mission. Period.\" Afterwards, he was advised that he doesn't have to read the punctuation marks.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Joke-id  \\\n",
       "137      138   \n",
       "138      139   \n",
       "\n",
       "                                                                                                                                                                                                                                                                      Text  \n",
       "137   WASHINGTON (Reuters) - A tragic fire on Monday destroyed the personal library of President George W. Bush. Both of his books have been lost. Presidential spokesman Ari Fleischer said the president was devastated, as he had not finished coloring the second one.  \n",
       "138                                                                                                  In a Veteran's Day speech, President Bush vowed, \"We will finish the mission. Period.\" Afterwards, he was advised that he doesn't have to read the punctuation marks.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke IDs:[60, 101]. Similarity score: 0.452866716935, Total votes: 4078\n",
      "Text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>What did the Buddhist say to the hot dog vendor? Make me one with everything.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>101</td>\n",
       "      <td>Did you hear about the Buddhist who refused Novocaine during a root canal? He wanted to transcend dental medication.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Joke-id  \\\n",
       "59        60   \n",
       "100      101   \n",
       "\n",
       "                                                                                                                      Text  \n",
       "59                                           What did the Buddhist say to the hot dog vendor? Make me one with everything.  \n",
       "100   Did you hear about the Buddhist who refused Novocaine during a root canal? He wanted to transcend dental medication.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke IDs:[58, 74]. Similarity score: 0.452843908367, Total votes: 3754\n",
      "Text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>How many teddy bears does it take to change a lightbulb? It takes only one teddy bear, but it takes a whole lot of lightbulbs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>Q: How many stalkers does it take to change a light bulb? A: Two. One to replace the bulb, and the other to watch it day and night.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Joke-id  \\\n",
       "57       58   \n",
       "73       74   \n",
       "\n",
       "                                                                                                                                    Text  \n",
       "57        How many teddy bears does it take to change a lightbulb? It takes only one teddy bear, but it takes a whole lot of lightbulbs.  \n",
       "73   Q: How many stalkers does it take to change a light bulb? A: Two. One to replace the bulb, and the other to watch it day and night.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_df=jokes_sim_df[jokes_sim_df[\"Total-votes\"] > 1000].sort([\"Sim-score\"],ascending=[0]).head(5)\n",
    "print \"Top 5 jokes having highest similarity scores and rated by atleast 1000 users\"\n",
    "display(display_df)\n",
    "#display(jokes_sim_df[jokes_sim_df[\"Total-votes\"] > 1000].sort([\"Sim-score\"],ascending=[0]).tail(10))\n",
    "#pd.merge(left=jokes_sim_df,right= items_df, ignore_index=True)\n",
    "for a, b, c, d in list(zip(display_df[\"Joke-1\"],\n",
    "                           display_df[\"Joke-2\"],\n",
    "                           display_df[\"Sim-score\"],\n",
    "                           display_df[\"Total-votes\"])):\n",
    "    print \"Joke IDs:[{}, {}]. Similarity score: {}, Total votes: {}\".format(a,b,c,d)\n",
    "    print \"Text:\"\n",
    "    display(items_df[((items_df[\"Joke-id\"] == a) | (items_df[\"Joke-id\"] == b))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the cosine similarity measure is working. We can infer the following:\n",
    "* The joke IDs 123 and 140 are about Chuck Norris\n",
    "* The joke IDs 86 and 94 are about atoms/neutrons/electrons. Observe that these jokes do not share any topic words. The ID 86 is about neutron, while ID 94 is about atom/electron\n",
    "* The joke IDs 138 and 139 are about President George W. Bush\n",
    "* The joke IDs 60 and 101 are about a Buddhist\n",
    "* The joke IDs 58 and 74 are about light bulbs. Note the spelling of \"light bulb\". In the 58 joke ID it is spelled as \"lightbulb\" while 74 ID has \"light bulb\". But cosine similarity was able to find that these 2 jokes are related to each other based on the user ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Top 5 dissimilar jokes**\n",
    "\n",
    "Let us display the top 5 jokes which are dissimilar to each other. Here also we will consider only the jokes that were rated by at least 1000 users in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 dissimilar scores, based on the cosine similarity score:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-1</th>\n",
       "      <th>Joke-2</th>\n",
       "      <th>Sim-score</th>\n",
       "      <th>Total-votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>106</td>\n",
       "      <td>124</td>\n",
       "      <td>-0.065407</td>\n",
       "      <td>3768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8486</th>\n",
       "      <td>106</td>\n",
       "      <td>141</td>\n",
       "      <td>-0.065697</td>\n",
       "      <td>3730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6592</th>\n",
       "      <td>114</td>\n",
       "      <td>141</td>\n",
       "      <td>-0.076192</td>\n",
       "      <td>3774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8042</th>\n",
       "      <td>127</td>\n",
       "      <td>141</td>\n",
       "      <td>-0.076766</td>\n",
       "      <td>3769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>57</td>\n",
       "      <td>127</td>\n",
       "      <td>-0.087814</td>\n",
       "      <td>4326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Joke-1  Joke-2  Sim-score  Total-votes\n",
       "1262     106     124  -0.065407         3768\n",
       "8486     106     141  -0.065697         3730\n",
       "6592     114     141  -0.076192         3774\n",
       "8042     127     141  -0.076766         3769\n",
       "4195      57     127  -0.087814         4326"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke IDs:[106, 124]. Similarity score: -0.065406866525, Total votes: 3768\n",
      "Text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>106</td>\n",
       "      <td>An engineer dies and reports to the pearly gates. St. Peter checks his dossier and says, \"Ah, you''re an engineer--you're in the wrong place.\" So, the engineer reports to the gates of hell and is let in. Pretty soon, the engineer gets dissatisfied with the level of comfort in hell, and starts designing and building improvements. After awhile, they've got air conditioning, flush toilets and escalators, and the engineer is a pretty popular guy. One day, God calls Satan up on the telephone and says with a sneer, \"So, how's it going down there in hell?\" Satan replies, \"Hey, things are going great. We've got air conditioning, flush toilets and escalators, and there's no telling what this engineer is going to come up with next.\" God replies, \"What?? You've got an engineer? That's a mistake--he should never have gotten down there; send him up here.\" Satan says, \"No way.\" I like having an engineer on the staff, and I'm keeping him.\" God says, \"Send him back up here or I'll sue.\" Satan laughs uproariously and answers, \"Yeah, right. And just where are YOU going to get a lawyer?\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>124</td>\n",
       "      <td>Person 1: Hey, wanna hear a great knock-knock joke? Person 2: Sure, What is it? Person 1: Okay, you start. Person 2: Knock-knock. Person 1: Who's there? Person 2: ... Person 1: Hah!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Joke-id  \\\n",
       "105      106   \n",
       "123      124   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Text  \n",
       "105   An engineer dies and reports to the pearly gates. St. Peter checks his dossier and says, \"Ah, you''re an engineer--you're in the wrong place.\" So, the engineer reports to the gates of hell and is let in. Pretty soon, the engineer gets dissatisfied with the level of comfort in hell, and starts designing and building improvements. After awhile, they've got air conditioning, flush toilets and escalators, and the engineer is a pretty popular guy. One day, God calls Satan up on the telephone and says with a sneer, \"So, how's it going down there in hell?\" Satan replies, \"Hey, things are going great. We've got air conditioning, flush toilets and escalators, and there's no telling what this engineer is going to come up with next.\" God replies, \"What?? You've got an engineer? That's a mistake--he should never have gotten down there; send him up here.\" Satan says, \"No way.\" I like having an engineer on the staff, and I'm keeping him.\" God says, \"Send him back up here or I'll sue.\" Satan laughs uproariously and answers, \"Yeah, right. And just where are YOU going to get a lawyer?\"  \n",
       "123                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Person 1: Hey, wanna hear a great knock-knock joke? Person 2: Sure, What is it? Person 1: Okay, you start. Person 2: Knock-knock. Person 1: Who's there? Person 2: ... Person 1: Hah!  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke IDs:[106, 141]. Similarity score: -0.06569676109, Total votes: 3730\n",
      "Text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>106</td>\n",
       "      <td>An engineer dies and reports to the pearly gates. St. Peter checks his dossier and says, \"Ah, you''re an engineer--you're in the wrong place.\" So, the engineer reports to the gates of hell and is let in. Pretty soon, the engineer gets dissatisfied with the level of comfort in hell, and starts designing and building improvements. After awhile, they've got air conditioning, flush toilets and escalators, and the engineer is a pretty popular guy. One day, God calls Satan up on the telephone and says with a sneer, \"So, how's it going down there in hell?\" Satan replies, \"Hey, things are going great. We've got air conditioning, flush toilets and escalators, and there's no telling what this engineer is going to come up with next.\" God replies, \"What?? You've got an engineer? That's a mistake--he should never have gotten down there; send him up here.\" Satan says, \"No way.\" I like having an engineer on the staff, and I'm keeping him.\" God says, \"Send him back up here or I'll sue.\" Satan laughs uproariously and answers, \"Yeah, right. And just where are YOU going to get a lawyer?\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>141</td>\n",
       "      <td>Jack Bauer can get McDonald's breakfast after 10:30.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Joke-id  \\\n",
       "105      106   \n",
       "140      141   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Text  \n",
       "105   An engineer dies and reports to the pearly gates. St. Peter checks his dossier and says, \"Ah, you''re an engineer--you're in the wrong place.\" So, the engineer reports to the gates of hell and is let in. Pretty soon, the engineer gets dissatisfied with the level of comfort in hell, and starts designing and building improvements. After awhile, they've got air conditioning, flush toilets and escalators, and the engineer is a pretty popular guy. One day, God calls Satan up on the telephone and says with a sneer, \"So, how's it going down there in hell?\" Satan replies, \"Hey, things are going great. We've got air conditioning, flush toilets and escalators, and there's no telling what this engineer is going to come up with next.\" God replies, \"What?? You've got an engineer? That's a mistake--he should never have gotten down there; send him up here.\" Satan says, \"No way.\" I like having an engineer on the staff, and I'm keeping him.\" God says, \"Send him back up here or I'll sue.\" Satan laughs uproariously and answers, \"Yeah, right. And just where are YOU going to get a lawyer?\"  \n",
       "140                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Jack Bauer can get McDonald's breakfast after 10:30.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke IDs:[114, 141]. Similarity score: -0.0761916493432, Total votes: 3774\n",
      "Text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>114</td>\n",
       "      <td>Sherlock Holmes and Dr. Watson go on a camping trip, set up their tent, and fall asleep. Some hours later, Holmes wakes his faithful friend. \"Watson, look up at the sky and tell me what you see.\"  Watson replies, \"I see millions of stars.\"  \"What does that tell you?\"  Watson ponders for a minute. \"Astronomically speaking, it tells me that there are millions of galaxies and potentially billions of planets. Astrologically, it tells me that Saturn is in Leo. Timewise, it appears to be approximately a quarter past three. Theologically, it's evident the Lord is all-powerful and we are small and insignificant. Meteorologically, it seems we will have a beautiful day tomorrow. What does it tell you?\"  Holmes is silent for a moment, then speaks. \"Watson, you idiot, someone has stolen our tent.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>141</td>\n",
       "      <td>Jack Bauer can get McDonald's breakfast after 10:30.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Joke-id  \\\n",
       "113      114   \n",
       "140      141   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Text  \n",
       "113   Sherlock Holmes and Dr. Watson go on a camping trip, set up their tent, and fall asleep. Some hours later, Holmes wakes his faithful friend. \"Watson, look up at the sky and tell me what you see.\"  Watson replies, \"I see millions of stars.\"  \"What does that tell you?\"  Watson ponders for a minute. \"Astronomically speaking, it tells me that there are millions of galaxies and potentially billions of planets. Astrologically, it tells me that Saturn is in Leo. Timewise, it appears to be approximately a quarter past three. Theologically, it's evident the Lord is all-powerful and we are small and insignificant. Meteorologically, it seems we will have a beautiful day tomorrow. What does it tell you?\"  Holmes is silent for a moment, then speaks. \"Watson, you idiot, someone has stolen our tent.\"  \n",
       "140                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Jack Bauer can get McDonald's breakfast after 10:30.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke IDs:[127, 141]. Similarity score: -0.0767660642306, Total votes: 3769\n",
      "Text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>127</td>\n",
       "      <td>A little boy goes to his dad and asks, \"What is politics?\" His dad says, \"Well son, let me try to explain it this way: I'm the breadwinner of the family, so let's call me capitalism. Your Mom, she's the administrator of the money, so we'll call her the government. We're here to take care of your needs, so we'll call you the people. The nanny, we'll consider her the working class. And your baby brother, we'll call him the future. Now, think about that and see if that makes sense.\" So the little boy goes off to bed thinking about what dad had said. Later that night, he hears his baby brother crying, so he gets up to check on him. He finds that the baby has severely soiled his diaper. So the little boy goes to his parents' room and finds his mother sound asleep. Not wanting to wake her, he goes to the nanny's room. Finding the door locked, he peeks in the keyhole and sees his father in bed with the nanny. He gives up and goes back to bed. The next morning, the little boy says to his father, \"Dad, I think I understand the concept of politics now.\" The father says, \"Good, son. Tell me in your own words what you think politics is all about.\" The little boy replies, \"Well, while capitalism is screwing the working class, the government is sound asleep, the people are being ignored and the future is in deep shit.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>141</td>\n",
       "      <td>Jack Bauer can get McDonald's breakfast after 10:30.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Joke-id  \\\n",
       "126      127   \n",
       "140      141   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Text  \n",
       "126   A little boy goes to his dad and asks, \"What is politics?\" His dad says, \"Well son, let me try to explain it this way: I'm the breadwinner of the family, so let's call me capitalism. Your Mom, she's the administrator of the money, so we'll call her the government. We're here to take care of your needs, so we'll call you the people. The nanny, we'll consider her the working class. And your baby brother, we'll call him the future. Now, think about that and see if that makes sense.\" So the little boy goes off to bed thinking about what dad had said. Later that night, he hears his baby brother crying, so he gets up to check on him. He finds that the baby has severely soiled his diaper. So the little boy goes to his parents' room and finds his mother sound asleep. Not wanting to wake her, he goes to the nanny's room. Finding the door locked, he peeks in the keyhole and sees his father in bed with the nanny. He gives up and goes back to bed. The next morning, the little boy says to his father, \"Dad, I think I understand the concept of politics now.\" The father says, \"Good, son. Tell me in your own words what you think politics is all about.\" The little boy replies, \"Well, while capitalism is screwing the working class, the government is sound asleep, the people are being ignored and the future is in deep shit.\"  \n",
       "140                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Jack Bauer can get McDonald's breakfast after 10:30.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joke IDs:[57, 127]. Similarity score: -0.087813975336, Total votes: 4326\n",
      "Text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>Why are there so many Jones's in the phone book? Because they all have phones.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>127</td>\n",
       "      <td>A little boy goes to his dad and asks, \"What is politics?\" His dad says, \"Well son, let me try to explain it this way: I'm the breadwinner of the family, so let's call me capitalism. Your Mom, she's the administrator of the money, so we'll call her the government. We're here to take care of your needs, so we'll call you the people. The nanny, we'll consider her the working class. And your baby brother, we'll call him the future. Now, think about that and see if that makes sense.\" So the little boy goes off to bed thinking about what dad had said. Later that night, he hears his baby brother crying, so he gets up to check on him. He finds that the baby has severely soiled his diaper. So the little boy goes to his parents' room and finds his mother sound asleep. Not wanting to wake her, he goes to the nanny's room. Finding the door locked, he peeks in the keyhole and sees his father in bed with the nanny. He gives up and goes back to bed. The next morning, the little boy says to his father, \"Dad, I think I understand the concept of politics now.\" The father says, \"Good, son. Tell me in your own words what you think politics is all about.\" The little boy replies, \"Well, while capitalism is screwing the working class, the government is sound asleep, the people are being ignored and the future is in deep shit.\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Joke-id  \\\n",
       "56        57   \n",
       "126      127   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Text  \n",
       "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Why are there so many Jones's in the phone book? Because they all have phones.  \n",
       "126   A little boy goes to his dad and asks, \"What is politics?\" His dad says, \"Well son, let me try to explain it this way: I'm the breadwinner of the family, so let's call me capitalism. Your Mom, she's the administrator of the money, so we'll call her the government. We're here to take care of your needs, so we'll call you the people. The nanny, we'll consider her the working class. And your baby brother, we'll call him the future. Now, think about that and see if that makes sense.\" So the little boy goes off to bed thinking about what dad had said. Later that night, he hears his baby brother crying, so he gets up to check on him. He finds that the baby has severely soiled his diaper. So the little boy goes to his parents' room and finds his mother sound asleep. Not wanting to wake her, he goes to the nanny's room. Finding the door locked, he peeks in the keyhole and sees his father in bed with the nanny. He gives up and goes back to bed. The next morning, the little boy says to his father, \"Dad, I think I understand the concept of politics now.\" The father says, \"Good, son. Tell me in your own words what you think politics is all about.\" The little boy replies, \"Well, while capitalism is screwing the working class, the government is sound asleep, the people are being ignored and the future is in deep shit.\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_df=jokes_sim_df[jokes_sim_df[\"Total-votes\"] > 1000].sort([\"Sim-score\"],ascending=[0]).tail(5)\n",
    "print \"Top 5 dissimilar scores, based on the cosine similarity score:\"\n",
    "display(display_df)\n",
    "#display(jokes_sim_df[jokes_sim_df[\"Total-votes\"] > 1000].sort([\"Sim-score\"],ascending=[0]).tail(10))\n",
    "#pd.merge(left=jokes_sim_df,right= items_df, ignore_index=True)\n",
    "for a, b, c, d in list(zip(display_df[\"Joke-1\"],display_df[\"Joke-2\"],display_df[\"Sim-score\"],\n",
    "                           display_df[\"Total-votes\"])):\n",
    "    print \"Joke IDs:[{}, {}]. Similarity score: {}, Total votes: {}\".format(a,b,c,d)\n",
    "    print \"Text:\"\n",
    "    display(items_df[((items_df[\"Joke-id\"] == a) | (items_df[\"Joke-id\"] == b))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Looking at the text of these joke pairs, we can infer that they are entirely different from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Making recommendations using cosine similarity measure\n",
    "Let us make recommendations for some of the users based on the jokes they rated high. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows of the ratings data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Joke_ID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>-9.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>-6.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User_ID  Joke_ID  Rating\n",
       "0        1        5   0.219\n",
       "1        1        7  -9.281\n",
       "2        1        8  -9.281\n",
       "3        1       13  -6.781\n",
       "4        1       15   0.875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Reading the ratings data set to a file\n",
    "ratings_df = pd.read_csv(\"jester_ratings.dat\",sep=\"\\t\\t\",header=None)\n",
    "ratings_df.columns = [\"User_ID\", \"Joke_ID\",\"Rating\"]\n",
    "print \"Initial rows of the ratings data:\"\n",
    "display(ratings_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a function that takes a user ID as input and recommends some jokes based on the jokes s/he has rated high. The recommended jokes were not already rated by the user. \n",
    "\n",
    "**NOTE** I assumed that a person likes a joke, whenever he gives a rating greater than 5. The default rating is 0 (see the website http://eigentaste.berkeley.edu/. The rating's slider bar stays at 0, if the user does not give any rating), +10 is the best possible rating and -10 is the least possible rating. So I chose 5 as the minimum rating to determine if the user really likes the jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_recommendations(user_id,ratings_df,top_n,jokes_sim_df,items_df):\n",
    "    #Get the top 5 jokes which the user has rated:\n",
    "    top_user_ratings = ratings_df[(ratings_df[\"User_ID\"] == user_id) & (ratings_df[\"Rating\"] > 5)\n",
    "                                 ].sort([\"Rating\"],ascending=[0]).head(5)\n",
    "    \n",
    "    print \"The user ID: {} has rated these jokes high:\".format(user_id)\n",
    "    display(top_user_ratings)\n",
    "    \n",
    "    #for a in list(top_user_ratings[\"Joke_ID\"]):\n",
    "        #display(items_df[items_df[\"Joke-id\"] == a])\n",
    "    display(items_df[items_df[\"Joke-id\"].isin(list(top_user_ratings[\"Joke_ID\"]))])    \n",
    "    #print \"Joke IDs:[{}, {}]. Similarity score: {}, Total votes: {}\".format(a,b,c,d)\n",
    "    #print \"Text:\"\n",
    "    #display(items_df[((items_df[\"Joke-id\"] == a) | (items_df[\"Joke-id\"] == b))])\n",
    "    \n",
    "    #Get the jokes which were already rated by the user\n",
    "    already_rated = list(ratings_df[(ratings_df[\"User_ID\"] == user_id)][\"Joke_ID\"])\n",
    "    recommend_list=list()\n",
    "    #Get the top recommendations for the user ID:\n",
    "    for i in list(top_user_ratings[\"Joke_ID\"]):\n",
    "        recommend_list.append(list(jokes_sim_df[\n",
    "                (jokes_sim_df[\"Joke-1\"] == i) & (jokes_sim_df[\"Total-votes\"] > 1000)\n",
    "                 & ~(jokes_sim_df[\"Joke-2\"].isin(already_rated))   \n",
    "                ].sort([\"Sim-score\"],ascending=[0]).head(1)[\"Joke-2\"]))\n",
    "        recommend_list.append(list(jokes_sim_df[\n",
    "                (jokes_sim_df[\"Joke-2\"] == i) & (jokes_sim_df[\"Total-votes\"] > 1000)\n",
    "                 & ~(jokes_sim_df[\"Joke-1\"].isin(already_rated))\n",
    "                ].sort([\"Sim-score\"],ascending=[0]).head(1)[\"Joke-1\"]))\n",
    "    recommend_list = [i for i in recommend_list if len(i) > 0]\n",
    "    #print recommend_list\n",
    "    recommend_list = [item for sublist in recommend_list for item in sublist]\n",
    "    #recommendations = list(recommendations.difference(already_rated))\n",
    "    \n",
    "    print \"Here are the recommended items for the user ID: {}\".format(user_id)\n",
    "    \n",
    "    display(items_df[items_df[\"Joke-id\"].isin(recommend_list)])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting recommendations for user ID 17 (just randomly selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user ID: 17 has rated these jokes high:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Joke_ID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>17</td>\n",
       "      <td>49</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>9.938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     User_ID  Joke_ID  Rating\n",
       "755       17       17  10.000\n",
       "771       17       49  10.000\n",
       "758       17       20  10.000\n",
       "766       17       35  10.000\n",
       "751       17        8   9.938"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Q. Did you hear about the dyslexic devil worshiper? A. He sold his soul to Santa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>How many men does it take to screw in a light bulb? One. Men will screw anything.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>What's the difference between a Macintosh and an Etch-a-Sketch? You don't have to shake the Mac to clear the screen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>An explorer in the deepest Amazon suddenly finds himself surrounded by a bloodthirsty group of natives. Upon surveying the situation, he says quietly to himself, \"Oh God, I'm screwed.\" The sky darkens and a voice booms out, \"No, you are NOT screwed. Pick up that stone at your feet and bash in the head of the chief standing in front of you.\" So with the stone he bashes the life out of the chief. He stands above the lifeless body, breathing heavily and looking at 100 angry natives... The voice booms out again, \"Okay....NOW you're screwed.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>Three engineering students were gathered together discussing the possible designers of the human body. One said, \"It was a mechanical engineer. Just look at all the joints.\" Another said, \"No, it was an electrical engineer. The nervous systems many thousands of electrical connections.\" The last said, \"Actually, it was a civil engineer. Who else would run a toxic waste pipeline through a recreational area?\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Joke-id  \\\n",
       "7         8   \n",
       "16       17   \n",
       "19       20   \n",
       "34       35   \n",
       "48       49   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Text  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Q. Did you hear about the dyslexic devil worshiper? A. He sold his soul to Santa.  \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 How many men does it take to screw in a light bulb? One. Men will screw anything.  \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                              What's the difference between a Macintosh and an Etch-a-Sketch? You don't have to shake the Mac to clear the screen.  \n",
       "34   An explorer in the deepest Amazon suddenly finds himself surrounded by a bloodthirsty group of natives. Upon surveying the situation, he says quietly to himself, \"Oh God, I'm screwed.\" The sky darkens and a voice booms out, \"No, you are NOT screwed. Pick up that stone at your feet and bash in the head of the chief standing in front of you.\" So with the stone he bashes the life out of the chief. He stands above the lifeless body, breathing heavily and looking at 100 angry natives... The voice booms out again, \"Okay....NOW you're screwed.\"  \n",
       "48                                                                                                                                         Three engineering students were gathered together discussing the possible designers of the human body. One said, \"It was a mechanical engineer. Just look at all the joints.\" Another said, \"No, it was an electrical engineer. The nervous systems many thousands of electrical connections.\" The last said, \"Actually, it was a civil engineer. Who else would run a toxic waste pipeline through a recreational area?\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the recommended items for the user ID: 17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>A guy walks into a bar and sits down next to an extremely gorgeous woman. The first thing he notices about her though, are her pants. They were skin-tight, high-waisted and had no obvious mechanism (zipper, buttons or velcro) for opening them.  After several minutes of puzzling over how she got the pants up over her hips, he finally worked up the nerve to ask her. \"Excuse me miss, but how do you get into your pants?\"  \"Well,\" she replied, \"you can start by buying me a drink.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>What is the difference between men and women? A woman wants one man to satisfy her every need. A man wants every woman to satisfy his one need.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>There was an engineer who had an exceptional gift for fixing all things mechanical. After serving his company loyally for over 30 years, he happily retired. Several years later the company contacted him regarding a seemingly impossible problem they were having with one of their multi-million dollar machines. They had tried everything and everyone else to get the machine fixed, but to no avail. In desperation, they called on the retired engineer who had solved so many of their problems in the past. The engineer reluctantly took the challenge. He spent a day studying the huge machine. At the end of the day, he marked a small \"x\" in chalk on a particular component of the machine and proudly stated \"This is where your problem is.\" The part was replaced and the machine worked perfectly again. The company received a bill for $50,000 from the engineer for his service. They demanded an itemized accounting of his charges. The engineer responded briefly One chalk mark: $1. Knowing where to put it: $49,999. He was paid in full and the engineer retired again in peace.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>What is the rallying cry of the International Dyslexic Pride movement? Dyslexics Untie!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>105</td>\n",
       "      <td>A couple of hunters are out in the woods in the deep south when one of them falls to the ground. He doesn't seem to be breathing, and his eyes are rolled back in his head. The other guy whips out his cell phone and calls 911. He gasps to the operator, \"My friend is dead! What can I do?\" The operator, in a calm and soothing voice, says, \"Alright, take it easy. I can help. First, let's make sure he's dead.\" There is silence, and then a gun shot is heard. The hunter comes back on the line. \"Okay. Now what??\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>126</td>\n",
       "      <td>A Briton, a Frenchman and a Russian are viewing a painting of Adam and Eve frolicking in the Garden of Eden. \"Look at their reserve, their calm,\" muses the Brit. \"They must be British.\" \"Nonsense,\" the Frenchman disagrees. \"They're naked, and so beautiful. Clearly, they are French.\" \"No way! They have no clothes and no shelter,\" the Russian points out, \"They have only an apple to eat, and they are being told they live in a paradise. Obviously, they are Russian.\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Joke-id  \\\n",
       "25        26   \n",
       "38        39   \n",
       "46        47   \n",
       "63        64   \n",
       "104      105   \n",
       "125      126   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Text  \n",
       "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    A guy walks into a bar and sits down next to an extremely gorgeous woman. The first thing he notices about her though, are her pants. They were skin-tight, high-waisted and had no obvious mechanism (zipper, buttons or velcro) for opening them.  After several minutes of puzzling over how she got the pants up over her hips, he finally worked up the nerve to ask her. \"Excuse me miss, but how do you get into your pants?\"  \"Well,\" she replied, \"you can start by buying me a drink.\"  \n",
       "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     What is the difference between men and women? A woman wants one man to satisfy her every need. A man wants every woman to satisfy his one need.  \n",
       "46    There was an engineer who had an exceptional gift for fixing all things mechanical. After serving his company loyally for over 30 years, he happily retired. Several years later the company contacted him regarding a seemingly impossible problem they were having with one of their multi-million dollar machines. They had tried everything and everyone else to get the machine fixed, but to no avail. In desperation, they called on the retired engineer who had solved so many of their problems in the past. The engineer reluctantly took the challenge. He spent a day studying the huge machine. At the end of the day, he marked a small \"x\" in chalk on a particular component of the machine and proudly stated \"This is where your problem is.\" The part was replaced and the machine worked perfectly again. The company received a bill for $50,000 from the engineer for his service. They demanded an itemized accounting of his charges. The engineer responded briefly One chalk mark: $1. Knowing where to put it: $49,999. He was paid in full and the engineer retired again in peace.  \n",
       "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             What is the rallying cry of the International Dyslexic Pride movement? Dyslexics Untie!  \n",
       "104                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     A couple of hunters are out in the woods in the deep south when one of them falls to the ground. He doesn't seem to be breathing, and his eyes are rolled back in his head. The other guy whips out his cell phone and calls 911. He gasps to the operator, \"My friend is dead! What can I do?\" The operator, in a calm and soothing voice, says, \"Alright, take it easy. I can help. First, let's make sure he's dead.\" There is silence, and then a gun shot is heard. The hunter comes back on the line. \"Okay. Now what??\"  \n",
       "125                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 A Briton, a Frenchman and a Russian are viewing a painting of Adam and Eve frolicking in the Garden of Eden. \"Look at their reserve, their calm,\" muses the Brit. \"They must be British.\" \"Nonsense,\" the Frenchman disagrees. \"They're naked, and so beautiful. Clearly, they are French.\" \"No way! They have no clothes and no shelter,\" the Russian points out, \"They have only an apple to eat, and they are being told they live in a paradise. Obviously, they are Russian.\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_recommendations(17,ratings_df,5,jokes_sim_df,items_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User ID 17 liked the joke ID 17 (about men), and the system has recommended joke ID 39 (which is also about men). Similarly the joke ID 8 (liked joke) and joke ID 64 (recommended joke) are related (both refer to Dyslexic). Jokes 20 (liked joke) and 39 (recommended joke) are also related, since both are worded similarly \"what is the difference between ... \". Joke ID 35 (liked joke) and 105 (recommended joke) are also related both refer to hunters/explorers, killing someone etc. Joke ID 49 (liked joke) and 47 (recommended joke) are related, since both refer to engineer(s). Joke ID 49 (liked joke) and 126 (recommended joke) ae also related, since both jokes refer to a series of reasons to determine \"who should be or have made something...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the recommendations for the user ID 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user ID: 1000 has rated these jokes high:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>Joke_ID</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38207</th>\n",
       "      <td>1000</td>\n",
       "      <td>15</td>\n",
       "      <td>9.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38208</th>\n",
       "      <td>1000</td>\n",
       "      <td>16</td>\n",
       "      <td>9.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38209</th>\n",
       "      <td>1000</td>\n",
       "      <td>17</td>\n",
       "      <td>9.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38210</th>\n",
       "      <td>1000</td>\n",
       "      <td>18</td>\n",
       "      <td>9.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38211</th>\n",
       "      <td>1000</td>\n",
       "      <td>19</td>\n",
       "      <td>9.031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User_ID  Joke_ID  Rating\n",
       "38207     1000       15   9.031\n",
       "38208     1000       16   9.031\n",
       "38209     1000       17   9.031\n",
       "38210     1000       18   9.031\n",
       "38211     1000       19   9.031"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Q: What did the blind person say when given some matzah? A: Who the hell wrote this?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Q. What is orange and sounds like a parrot? A. A carrot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>How many men does it take to screw in a light bulb? One. Men will screw anything.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>A dog walks into Western Union and asks the clerk to send a telegram. He fills out a form on which he writes down the telegram he wishes to send: \"Bow wow wow, bow wow wow.\" The clerk says, \"You can add another 'Bow wow' for the same price.\" The dog responded, \"Now wouldn't that sound a little silly?\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Q: If a person who speaks three languages is called \"trilingual,\" and a person who speaks two languages is called \"bilingual,\" what do you call a person who only speaks one language? A: American!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Joke-id  \\\n",
       "14       15   \n",
       "15       16   \n",
       "16       17   \n",
       "17       18   \n",
       "18       19   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                               Text  \n",
       "14                                                                                                                                                                                                                             Q: What did the blind person say when given some matzah? A: Who the hell wrote this?  \n",
       "15                                                                                                                                                                                                                                                         Q. What is orange and sounds like a parrot? A. A carrot.  \n",
       "16                                                                                                                                                                                                                                How many men does it take to screw in a light bulb? One. Men will screw anything.  \n",
       "17   A dog walks into Western Union and asks the clerk to send a telegram. He fills out a form on which he writes down the telegram he wishes to send: \"Bow wow wow, bow wow wow.\" The clerk says, \"You can add another 'Bow wow' for the same price.\" The dog responded, \"Now wouldn't that sound a little silly?\"  \n",
       "18                                                                                                              Q: If a person who speaks three languages is called \"trilingual,\" and a person who speaks two languages is called \"bilingual,\" what do you call a person who only speaks one language? A: American!  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the recommended items for the user ID: 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke-id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>What do you get when you run over a parakeet with a lawnmower? Shredded tweet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>What is the difference between men and women? A woman wants one man to satisfy her every need. A man wants every woman to satisfy his one need.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>A horse walks into a bar. The bartender asks \"So, why the long face?\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>Why are there so many Jones's in the phone book? Because they all have phones.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>138</td>\n",
       "      <td>WASHINGTON (Reuters) - A tragic fire on Monday destroyed the personal library of President George W. Bush. Both of his books have been lost. Presidential spokesman Ari Fleischer said the president was devastated, as he had not finished coloring the second one.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Joke-id  \\\n",
       "23        24   \n",
       "38        39   \n",
       "43        44   \n",
       "56        57   \n",
       "137      138   \n",
       "\n",
       "                                                                                                                                                                                                                                                                      Text  \n",
       "23                                                                                                                                                                                          What do you get when you run over a parakeet with a lawnmower? Shredded tweet.  \n",
       "38                                                                                                                         What is the difference between men and women? A woman wants one man to satisfy her every need. A man wants every woman to satisfy his one need.  \n",
       "43                                                                                                                                                                                                   A horse walks into a bar. The bartender asks \"So, why the long face?\"  \n",
       "56                                                                                                                                                                                          Why are there so many Jones's in the phone book? Because they all have phones.  \n",
       "137   WASHINGTON (Reuters) - A tragic fire on Monday destroyed the personal library of President George W. Bush. Both of his books have been lost. Presidential spokesman Ari Fleischer said the president was devastated, as he had not finished coloring the second one.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_recommendations(1000,ratings_df,5,jokes_sim_df,items_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user ID 1000 liked the joke ID 18 (about a dog), and the system has recommended joke ID 44 (about a horse). Also it looks like the user like jokes which are more like interrogative statements or questions, and the recommended jokes are also of the same form (except the joke ID 138). \n",
    "\n",
    "Given the above recommendations, we can infer that the cosine similarity is doing a descent job. However, we have the problems:\n",
    "\n",
    "1. For a new user we do not have a good mechanism to provide recommendations (the cold start problem).\n",
    "\n",
    "2. We assumed that a rating of greater than 5 implies that the user has really liked the joke. This assumption might be incorrect. Based on this criteria, we may not have any jokes to recommend, if a user gives a rating of less than 5 for all the jokes s/he reads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent \n",
    "We will now evaluate the SGD (Stochastic Gradient Descent) method to factorize the user-item ratings (utility matrix) to predict the ratings a user could give to a joke he never read.\n",
    "\n",
    "In recommendation systems, we have a Utility matrix that shows the affinity of all users towards all available items. But this Utility matrix is very sparse, since most of the users might not have looked or experienced all items, and the main goal of recommender systems is to identify the potential items the user might be interested in. One way to accomplish this goal is based on matrix factorization method. We have to express the Utility matrix as a product of two matrices, to estimate the missing entries in the Utility matrix. In mathematical terms, we define our main objective as given below. \n",
    "\n",
    "_Objective_: Express the matrix M as a product of U and V. Where _M_ is a _mXn_ matrix, _U_ is a _mXd_ matrix and _V_ is a _dXn_ matrix, and _d_ is the number of latent factors. \n",
    "\n",
    "\n",
    "Mathematically, let\n",
    "$$M=\\left[ \\begin{array}{cccccc} r_{11} & r_{12} & . & . & r_{1n} \\\\ r_{21} & r_{22} & . & . & r_{2n} \\\\ r_{31} & r_{32} & . & . & r_{3n} \\\\ . & . & . & . & . \\\\  . & . & . & . & . \\\\ r_{m1} & r_{m2} & . & . & r_{mn} \\end{array} \\right]$$\n",
    "$$U=\\left[ \\begin{array}{ccc} u_{11} & . & u_{1d} \\\\ u_{21} & . & u_{2d} \\\\  u_{31} & . & u_{3d} \\\\ . & . & . \\\\ .& . & . \\\\ u_{m1} & . & u_{md} \\end{array} \\right]$$ \n",
    "$$V=\\left[ \\begin{array}{ccccc} v_{11} & v_{12} & . & . & v_{1n} \\\\ v_{21} & v_{22} & . & . & v_{2n} \\\\ . & . & . & . & . \\\\v_{d1} & v_{d2} & . & . & v_{dn}\\end{array} \\right]\n",
    "$$\n",
    "\n",
    "Then M can be expressed as the matrix product of U and V, as shown below: \n",
    "$$\\left[ \\begin{array}{cccccc} r_{11} & r_{12} & . & . & r_{1n} \\\\ r_{21} & r_{22} & . & . & r_{2n} \\\\ r_{31} & r_{32} & . & . & r_{3n} \\\\ . & . & . & . & . \\\\  . & . & . & . & . \\\\ r_{m1} & r_{m2} & . & . & r_{mn} \\end{array} \\right] \\approx \n",
    "\\left[ \\begin{array}{ccc} u_{11} & . & u_{1d} \\\\ u_{21} & . & u_{2d} \\\\  u_{31} & . & u_{3d} \\\\ . & . & . \\\\ .& . & . \\\\ u_{m1} & . & u_{md} \\end{array} \\right] \\left[ \\begin{array}{ccccc} v_{11} & v_{12} & . & . & v_{1n} \\\\ v_{21} & v_{22} & . & . & v_{2n} \\\\ . & . & . & . & . \\\\v_{d1} & v_{d2} & . & . & v_{dn}\\end{array} \\right] $$\n",
    "\n",
    "In recommender systems, the matrix M is a sparse matrix with many unknown entries. An example of such sparse matrix is shown below:\n",
    "\n",
    "$$M=\\left[ \\begin{array}{cccccc}  & r_{12} & . & . & r_{1n} \\\\ r_{21} &   & . & . & r_{2n} \\\\ r_{31} &   & . & . &   \\\\ . & . & . & . & . \\\\  . & . & . & . & . \\\\ r_{m1} & r_{m2} & . & . &   \\end{array} \\right]$$\n",
    "\n",
    "For such sparse matrices, we cannot use SVD (Singular Value Decomposition) method to factorize the matrix. Hence for recommendation systems, our main objective is to estimate the U and V matrices considering only the available data in M. Once we obtain optimal U and V matrices (based on the available data in M), we can get the matrix product UV, and estimate an approximate value of the missing elements in M, by comparing the corresponding elements between M and UV. Another advantage of U, V factorization is to identify the hidden dimensions (also called latent factors), which map both the user and items to a common set of dimensions/coordinate system. Such mapping will help to identify users/items/user-item pairs, which are near to each other.\n",
    "\n",
    "Two of the prominent methods to estimate the U and V matrices are: \n",
    "\n",
    "* Alternating Least Squares (ALS) \n",
    "\n",
    "* Gradient descent method (Stochastic Gradient Descent or SGD)\n",
    "\n",
    "Spark MLLib has a built-in ALS implementation. But for this project we will use SGD implemented on a non-distributed environment. The goal is to check if SGD can be used on the 1.7 Million ratings data set in a non-distributed environment (a laptop with 16GB RAM), and evaluate the performance of the SGD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD algorithm\n",
    "\n",
    "1. Initialize U and V to random values. We can assume 0s for NA values in M, use SVD method to obtain U and V, and use these values as the initial values of U and V.\n",
    "\n",
    "2. Randomly choose U or V\n",
    "\n",
    "3. If U is chosen, randomly choose a row _i_ from U. To estimate the row $U_i$, perform the following:\n",
    "\n",
    "   3a. Get the list of all columns in $M_i$ row, where we have available values. Call these locations as C\n",
    "\n",
    "   3b. Let $V_C = V[:,C]$, where $V[:,C]$ is the list of all columns in V, corresponding to the column numbers present in C\n",
    "   \n",
    "   3c. Estimate new value of $U_i$ as:\n",
    "$$U^{new}_{i} = U_{i} - \\alpha \\nabla_{U_{i}} f_{ij}(U_i,V_i)$$ where $\\alpha$ is the learning rate, and $\\nabla_{U_i}f_{ij}(U_i,V_j)$ is defined as follows:\n",
    "$$\\nabla_{U_i}f_{ij}(U_i,V_j) = [M_i - U_i.V_C].V_C^T + \\lambda_1 U_i$$ where $\\lambda_1$ is the regularization parameter\n",
    "\n",
    "4. If V is chosen, randomly choose a column _j_ from V. To estimate the column $V_j$, perform the following:\n",
    "\n",
    "   4a. Get the list of all rows in $M_j$ column, where we have available values. Call these locations as R\n",
    "\n",
    "   4b. Let $U_R = U[R,:]$, where $U[R,:]$ is the list of all rows in U, corresponding to the row numbers present in R\n",
    "   \n",
    "   4c. Estimate new value of $V_j$ as:\n",
    "$$V^{new}_{j} = V_{j} - \\alpha \\nabla_{V_{j}} f_{ij}(U_i,V_i)$$ where $\\alpha$ is the learning rate, and $\\nabla_{V_{j}}f_{ij}(U_i,V_j)$ is defined as follows:\n",
    "$$\\nabla_{V_{j}}f_{ij}(U_i,V_j) = [M_j - U_R.V_j]^T.U_R + \\lambda_2 V_j$$ where $\\lambda_2$ is the regularization parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of SGD\n",
    "\n",
    "We will code the following 4 functions functions to compute U and V using SGD method:\n",
    "\n",
    "**SGD_factorization(M,d, lambda1, lambda2, n, error_diff, seed,alpha)**\n",
    "In SGD we will estimate the row of U or column of V by choosing randomly U or V and choosing the row from U or column from V randomly. The randomness in choosing the elements is important. The function SGD_factorization function will accept 8 parameters. The first parameter is M (utility matrix), the second parameter is _d_ (the desired number of latent factors), the third parameter is _lambda1_ (regularization factor for computing U's row), _lambda2_ (regularization factor for computing V's column), _n_ is the maximum number of iterations, and *error_diff* is the least acceptable error difference between the consecutive iterations, _seed_ will accept a number which can be used to reproduce the results, and _alpha_ is the learning rate. The algorithm stops updating the elements of U and V once the specified number of iterations is reached or when the error difference between consecutive iterations is less than or equal to *error_diff*\n",
    "\n",
    "\n",
    "**SGD_compute_U_row(i,M,V,d,lambda1)**\n",
    "This function will accept 5 variables as inputs, and computes the gradient for a specific row in U, while keeping all other U and V elements constant. The parameters details are given below:\n",
    "\n",
    "i = desired row number in U that needs to be estimated (row numbers begin from 0)\n",
    "\n",
    "M = Utility matrix, which needs to be factorized\n",
    "\n",
    "V = V matrix or factor in the expression M = UV\n",
    "\n",
    "d = desired number of latent factors or columns in U\n",
    "\n",
    "lambda1 = regularization parameter\n",
    "\n",
    "\n",
    "**SGD_compute_V_col(j,M,U,d,lambda2)**\n",
    "This function will also accept 5 variables as inputs, and computes the gradient for a specific column in V, while keeping all other U and V elements constant. The parameters details are given below:\n",
    "\n",
    "j = desired column number in V that needs to be estimated (column numbers begin from 0)\n",
    "\n",
    "M = Utility matrix, which needs to be factorized\n",
    "\n",
    "U = U matrix or factor in the expression M = UV\n",
    "\n",
    "d = desired number of latent factors or rows in V\n",
    "\n",
    "lambda2 = regularization parameter\n",
    "\n",
    "**get_RMSE_error(M,U,V)**\n",
    "This function will compute the RMSE between M and UV, considering the available elements only in M. Takes 3 parameters M, U and V. M is the Utility matrix, U is the U component and V is the V component in M = UV. \n",
    "\n",
    "The following block implements the SGD algorithm in Python 2.7. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_RMSE(M,U,V):\n",
    "    return np.sqrt(np.nanmean(np.square(M - np.dot(U,V))))\n",
    "\n",
    "def SGD_compute_U_row(i,M,U,V,d,lambda1,alpha):\n",
    "    '''\n",
    "     i = desired row number in U, which needs to be computed\n",
    "     M = Utility matrix of size mXn\n",
    "     V = V component of size dXn\n",
    "     lambda1 = reg factor\n",
    "    '''\n",
    "    num_of_rows, num_of_cols = M.shape\n",
    "   \n",
    "    #np.where will return a tuple.\n",
    "    C = np.where(~np.isnan(M[i,]))[0]\n",
    "    \n",
    "    V_C = V[:,C].copy()\n",
    "    M_i = M[i,:].copy()\n",
    "    M_i = M_i[~np.isnan(M_i)]\n",
    "    return alpha*(-1*np.dot((M_i - np.dot(U[i,],V_C)),V_C.T) + lambda1 * U[i,])\n",
    "    \n",
    "def SGD_compute_V_col(j,M,U,V,d,lambda2,alpha):\n",
    "    '''\n",
    "     j = desired column number in V, which needs to be computed\n",
    "     M = Utility matrix of size mXn\n",
    "     U = U component of size mXd\n",
    "     lambda2 = reg factor\n",
    "    '''\n",
    "    num_of_rows, num_of_cols = M.shape\n",
    "    \n",
    "    #np.where will return a tuple.\n",
    "    R = np.where(~np.isnan(M[:,j]))[0]\n",
    "    \n",
    "    U_R = U[R,:].copy()\n",
    "    M_j = M[:,j].copy()\n",
    "    M_j = M_j[~np.isnan(M_j)]\n",
    "    return alpha*(-1*np.dot((M_j - np.dot(U_R,V[:,j])).T,U_R) + lambda2 * V[:,j])\n",
    "\n",
    "\n",
    "def SGD_factorization(M,d, lambda1, lambda2, n, error_diff, seed,alpha):\n",
    "    import numpy as np\n",
    "    #Initialize U and V\n",
    "    M_rows, M_cols = M.shape\n",
    "    M_non_nan = np.nan_to_num(M)\n",
    "\n",
    "    U, s, vt = svds(M_non_nan, k=d, ncv=None, tol=0, which='LM', \n",
    "                             v0=None, maxiter=None, return_singular_vectors=True)\n",
    "    #print U\n",
    "    #print np.dot(np.diag(s),vt)\n",
    "    #print vt\n",
    "    V = np.dot(np.diag(s),vt)\n",
    "    #print \"In SGD Factorization...\"\n",
    "    #print \"U.shape:{}\".format(U.shape)\n",
    "    #print \"V.shape:{}\".format(V.shape)\n",
    "    \n",
    "    #print V\n",
    "    #np.random.seed(seed)\n",
    "    #U = np.zeros((M_rows,d),dtype=np.float)+np.random.rand(M_rows,d)\n",
    "    #np.random.seed(seed)\n",
    "    #V = np.zeros((d,M_cols),dtype=np.float)+np.random.rand(d,M_cols)\n",
    "    error = []\n",
    "    error.append(get_RMSE(M,U,V))\n",
    "\n",
    "    for count in xrange(n):\n",
    "            #Do not use any seed here\n",
    "            #pick_1 = np.random.random_integers(0,1)\n",
    "            pick_1 = np.random.randint(0,2)\n",
    "            if pick_1 == 0:\n",
    "                #pick_2 = np.random.random_integers(0,M_rows - 1)\n",
    "                pick_2 = np.random.randint(0,M_rows)\n",
    "                U[pick_2,] = U[pick_2,] - SGD_compute_U_row(pick_2,M,U,V,d,lambda1,alpha)\n",
    "                error.append(get_RMSE(M,U,V))\n",
    "                if np.absolute(error[-2] - error[-1]) <= error_diff:\n",
    "                    return [U, V, error]\n",
    "            else:\n",
    "                #pick_2 = np.random.random_integers(0,M_cols-1)\n",
    "                pick_2 = np.random.randint(0,M_cols)\n",
    "                V[:,pick_2] = V[:,pick_2] - SGD_compute_V_col(pick_2,M,U,V,d,lambda2,alpha)\n",
    "                error.append(get_RMSE(M,U,V))        \n",
    "                if np.absolute(error[-2] - error[-1]) <= error_diff:\n",
    "                    return [U, V, error]\n",
    "    return [U, V, error]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the framework to measure the SGD's performance \n",
    "\n",
    "We need to build the following functions to successfully evaluate the algorithm's performance:\n",
    "\n",
    "* Select a desired number of users randomly. This will help us to test the scalability of our algorithm for different volumes of data, since number of users selected is proportional to the amount of data used.\n",
    "\n",
    "* Create a Utility matrix based on the selected users-rating data.\n",
    "\n",
    "* The Utility matrix, will be split into test and training data (20:80 for test:training respectively). A function will be written to perform this split. If only one rating is available for an item, the function must eliminate such item's rating from test data. The split must be random.\n",
    "\n",
    "* Define a function to obtain the ratings present at the intersection of User-Items of the Utility matrix.\n",
    "\n",
    "* Define a function to normalize the data.\n",
    "\n",
    "* Define a function to plot the ROC curves.\n",
    "\n",
    "* Use the above functions and evaluate the algorithm's performance. The following metrics will be used to compare the algorithm's performance for various parameters of the algorithm:\n",
    "    * RMSE (of both the test and training data)\n",
    "    * Run time \n",
    "    * AUC (Area Under the Curve) in ROC\n",
    "\n",
    "\n",
    "### Selecting the users randomly\n",
    "\n",
    "To test the scalability of our algorithm, we have to select different amounts of training data, so that the algorithm's performance can be compared by recording the runtimes for different volumes of training data. To achieve this requirement, let us define a python function that randomly selects the data belonging to a given number of users. \n",
    "\n",
    "**NOTE:**\n",
    "We will NOT use the following function in this project, since we will use the complete data to train and test our algorithm. However I included this algorithm so that we have a complete frame work to develop SGD algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_data(user_df, n=10,seed=1234):\n",
    "    #Selecting only n users randomly\n",
    "    #Set the seed, to seed to reproduce the same results\n",
    "    np.random.seed(seed)\n",
    "    uids = np.random.randint(1,user_df[\"user_id\"].max(),n)\n",
    "\n",
    "    user_df=user_df.iloc[uids]\n",
    "    uids = [i+1 for i in uids]\n",
    "\n",
    "    #Combined data frame\n",
    "    df = pd.merge(pd.merge(user_df,ratings_df),movie_df)\n",
    "    #print \"All columns combined (sample records):\"\n",
    "    #display(df.head())\n",
    "\n",
    "    return [df, user_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building utility matrix\n",
    "The following function will build the utility matrix. If a user has rated the same joke more than once, then we will take the average of such ratings to represent the user rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_utility_matrix(df):\n",
    "        \n",
    "        ##Confine the columns to just user ID, Item ID, and rating only\n",
    "        df_final = df[[\"user_id\",\"item_id\",\"rating\"]]\n",
    "\n",
    "        #Some users have rated the same movie multiple times. So taking mean of such ratings\n",
    "        df_final = df_final.groupby([\"user_id\",\"item_id\"]).mean()\n",
    "        df_final = df_final.reset_index()\n",
    "\n",
    "        #Building the utility matrix\n",
    "        Utility = df_final.pivot(index=\"user_id\",columns=\"item_id\",values=\"rating\")\n",
    "        Utility.columns.names=[\" \"]\n",
    "        return Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into test and train data sets\n",
    "We will create a function that takes Utility matrix as input, along with the test data's size (expressed in percentage), and returns test and training data as output.\n",
    "\n",
    "The function will first identify the cells in the utility matrix where a joke has been rated by at least 2 users, and randomly pick the desired percentage of such cells as test data. This way we can eliminate the chance of picking any joke rated by single user as test data. So unless a joke has atleast 2 ratings, we will not consider that joke to test our algorithm's performance.\n",
    "\n",
    "NOTE: In these functions, we assume that the rows of Utility matrix represent the user IDs and the columns represent the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a function that gets the ratings values at the \n",
    "#intersection of row_idx and col_idx values, where\n",
    "#row_idx and col_idx are lists\n",
    "\n",
    "def get_ratings(Utility,row_idx,col_idx,indices=True):\n",
    "    '''\n",
    "       row_idx and col_idx are lists containing the indices of the utility matrix.\n",
    "       If indices=True, then the row_idx and col_idx represent the actual index value and column name\n",
    "       else, they represent the row number and column number respectively. \n",
    "        \n",
    "    '''\n",
    "    ratings = list()\n",
    "     \n",
    "    if len(row_idx) == len(col_idx):\n",
    "        if indices:\n",
    "            for i in xrange(len(col_idx)):\n",
    "                ratings.append(Utility.loc[row_idx[i],col_idx[i]])\n",
    "        else:\n",
    "            for i in xrange(len(col_idx)):\n",
    "                ratings.append(Utility.iloc[row_idx[i],col_idx[i]])\n",
    "        return ratings\n",
    "    else:\n",
    "        print \"Error. The lengths of the row and col locations must be same\"\n",
    "\n",
    "\n",
    "def split_data(Utility,test_perc=20):\n",
    "        #What number makes the 20% of the ratings?\n",
    "        #Find the cell locations where there is a true rating.\n",
    "        rows,cols=np.where(~np.isnan(Utility))\n",
    "\n",
    "        #What percentage of cells have the true rating?\n",
    "        nan_perc = 100-100*float(len(cols))/(Utility.shape[0]*Utility.shape[1])\n",
    "        non_nan_perc = 100*float(len(cols))/(Utility.shape[0]*Utility.shape[1])\n",
    "        \n",
    "        #Find the number of observations needed for the test data\n",
    "        test_number = np.trunc(test_perc / 100.0 * len(cols))\n",
    "        \n",
    "        #Find the locations (row,col) in the Utility matrix\n",
    "        #wherever we have a genuine value. The ratings_locations data frame (defined below)\n",
    "        #will have these (row-column) details.\n",
    "        ratings_locations = pd.DataFrame(zip(rows,cols),columns = [\"row\",\"column\"])\n",
    "\n",
    "        #Get the column locations which have at least 2 ratings.\n",
    "        #This will make sure that we do not accidentally select a \n",
    "        #rating (which is the only rating available for the item or row) \n",
    "        ratings_counts = ratings_locations.groupby([\"column\"])['row'].count()\n",
    "        #print ratings_counts\n",
    "        \n",
    "        #display(ratings_counts)\n",
    "        cols_num_2_ratings = list(ratings_counts[ratings_counts>1].index)\n",
    "        test_row_num = list()\n",
    "        test_col_num = list()\n",
    "\n",
    "        sample_count = 0\n",
    "        \n",
    "        train_df = Utility.copy()\n",
    "        for i in cols_num_2_ratings:\n",
    "            #The random choice is important. It helps us to randomly \n",
    "            #select the row location so that we do not select the value from the \n",
    "            #same row location.\n",
    "            #test_row_num.append(np.random.choice(np.where(~np.isnan(Utility.iloc[:,i]))[0]))\n",
    "            #Get the location of the row and columns into variables\n",
    "            temp_test_row = np.random.choice(np.where(~np.isnan(Utility.iloc[:,i]))[0])\n",
    "            #test_col_num.append(i)\n",
    "            temp_test_col = i\n",
    "            #Keep a track of the sample size\n",
    "            sample_count = sample_count + 1\n",
    "            \n",
    "            #If a user-item pair is selected for test, make that value as NA in training data \n",
    "            #However, there is a chance that the whole row could be NA, after this operation.\n",
    "            #So save the current value in tem_value, and \n",
    "            #re-assign to the same row-column value in train_df.\n",
    "            #temp_value=train_df.iloc[test_row_num[-1],test_col_num[-1]]\n",
    "            temp_value=train_df.iloc[temp_test_row,temp_test_col]\n",
    "            #train_df.iloc[test_row_num[-1],test_col_num[-1]] = np.nan\n",
    "            train_df.iloc[temp_test_row,temp_test_col] = np.nan\n",
    "            #if np.isnan(train_df.iloc[test_row_num[-1],:]).all():\n",
    "            ##Undo the change in training data, if the change results in all NAs in the row\n",
    "            if np.isnan(train_df.iloc[temp_test_row,:]).all():\n",
    "                #print \"Encountered the situation of having all values in train as NA.\"\n",
    "                #print \"So reduced the count of sample number\"\n",
    "                sample_count = sample_count - 1\n",
    "                #train_df.iloc[test_row_num[-1],test_col_num[-1]] = temp_value\n",
    "                train_df.iloc[temp_test_row,temp_test_col] = temp_value\n",
    "                continue\n",
    "            test_row_num.append(temp_test_row)\n",
    "            test_col_num.append(temp_test_col)\n",
    "            \n",
    "            #Stop data selection once we obtain the desired number of samples\n",
    "            if sample_count >= test_number:\n",
    "                break\n",
    "            \n",
    "        \n",
    "        ##Get the ratings at the intersection of row and column numbers from the \n",
    "        ##Utility matrix\n",
    "        test_ratings=get_ratings(Utility,test_row_num,test_col_num,indices=False)\n",
    "\n",
    "        ##Prepare the test data frame\n",
    "        user_id = [Utility.index[i] for i in test_row_num]\n",
    "        item_id = [Utility.columns[i] for i in test_col_num]\n",
    "\n",
    "        test_df = pd.DataFrame(zip(test_row_num,test_col_num,\n",
    "                           user_id,\n",
    "                           item_id,\n",
    "                           test_ratings),columns=[\"row_number\",\"column_number\",\"user_id\",\"item_id\",\"rating\"])\n",
    "        \n",
    "        return [train_df,test_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the Utility matrix\n",
    "\n",
    "We will define a function _normalize(M)_ that performs the normalization of Utility matrix _M_ based on the following logic. Normalization helps us to eliminate user and item biases.\n",
    "\n",
    "*normalize(M):* This function takes numpy matrix M as input and normalizes the matrix's data using the following logic:\n",
    "1. For each row, get the respective mean (ignoring the NANs).\n",
    "2. Subtract the means obtained in step-1 from the respective rows\n",
    "3. Get the means of the columns of the modified matrix from step-2\n",
    "4. Subtract the column means from the respective columns of the modified matrix, obtained in step-2\n",
    "\n",
    "**NOTE:**\n",
    "The following block implements the normalization in Python. But we also used the same normalization technique in PySpark also. Refer to *Appendix-C* for the implementation of normalization using PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Normalize the Utility matrix:\n",
    "##Subtract the avg user rating and avg item rating from the item in M\n",
    "#print M\n",
    "def list_mean(l):\n",
    "    if np.isnan(l).all():\n",
    "        print \"list_mean() message: All NA values. Check the logic.\"\n",
    "    return np.nanmean(l)\n",
    "\n",
    "def normalize(M):\n",
    "    #Convert M to a numpy array\n",
    "    #print \"In normalize\"\n",
    "    M = np.array(M)\n",
    "    \n",
    "    #Get the column means (or items mean)\n",
    "    items_mean=np.apply_along_axis(list_mean,0,M)\n",
    "    \n",
    "    #If an item is NOT rated by any user, then we will get NA for mean\n",
    "    #So for such instances, fill the mean item ratings with 0\n",
    "    #Such scenario may arise if only one user has rated a movie,\n",
    "    #and if that instance is included in the test data.\n",
    "    #items_mean = np.nan_to_num(items_mean)\n",
    "    #items_mean[np.where(np.isnan(items_mean))] = np.nanmean(M)\n",
    "    #print \"In normalize items mean\"\n",
    "    #print items_mean\n",
    "    if np.isnan(np.sum(items_mean)):\n",
    "        print \"WARNING: Items has NAN values. which is incorrect\"\n",
    "    #print \"items related nan.{}\".format(np.isnan(np.sum(items_mean)))\n",
    "    #print np.where(np.isnan(items_mean))\n",
    "    #print M[:,20]\n",
    "    #print len(items_mean)\n",
    "    \n",
    "    #Subtract the columns mean from the respective columns\n",
    "    M_normalized = M[:,] - items_mean\n",
    "    \n",
    "    #Get the rows means (or users mean) using the partially normalized matrix\n",
    "    #users_mean = np.apply_along_axis(list_mean,1,M_normalized)\n",
    "    users_mean = np.apply_along_axis(list_mean,1,M)\n",
    "    if np.isnan(np.sum(users_mean)):\n",
    "        print \"WARNING: Users has NAN values. which is incorrect\"\n",
    "\n",
    "    #print \"users related nan.{}\".format(np.isnan(np.sum(users_mean)))\n",
    "    #print \"In normalize users mean\"\n",
    "    #print np.isnan(np.sum(users_mean))\n",
    "    #print len(users_mean)\n",
    "    #Subtract the rows means\n",
    "    M_normalized = (M_normalized[:,].T- users_mean)\n",
    "    \n",
    "    #Transform back\n",
    "    return [M_normalized.T, items_mean, users_mean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms evaluation\n",
    "For SGD algorithm, we have the following parameters to tune:\n",
    "\n",
    "* $\\lambda_1$: The regularization parameter while finding U matrix\n",
    "* $\\lambda_2$: The regularization parameter while finding V matrix\n",
    "* $d$: Desired number of latent factors\n",
    "* $n$: Number of users to select. This indirectly controls the volume of the training data\n",
    "\n",
    "* In our evaluation, we will use the same values for $\\lambda_1$ and $\\lambda_2$. We will use the following values for these parameters:\n",
    "\n",
    "$$\\lambda_{1,2} = [0.1,1,2,3,4,5,10,100]$$\n",
    "\n",
    "* We will select $n$ as $59132$. We have a maximum of 59132 users in the data set\n",
    "\n",
    "* Out of the _n_ users ratings, 80% of data will be used for training and 20% for testing. The training data and test data are randomly selected\n",
    "\n",
    "* The value of $d$ (latent factors) will have the following values:\n",
    "$$d=[2,5,10,15,20,25,30,35,40,50,60,80,100]$$\n",
    "\n",
    "* We will use a constant learning rate of 0.00001\n",
    "\n",
    "* For each parameter combination, we will get the test and training error\n",
    "\n",
    "* The experiment is repeated 5 times. For each iteration, we select 80% of the data for training and 20% of the data for testing\n",
    "\n",
    "* The average of test and training errors for all the parameters combinations in all the iterations are obtained. The least average test error is finally picked as the optimal set of parameters to implement the SGD algorithm for the Jokes data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "Do NOT execute the following code, since this code will run for a while. It ran for 1 hour on a computer with 16GB RAM. To save time, I saved the performance metrics of various parameters combinations in a file named \"performance_df.csv\". This file can be downloaded from the github location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training episode: 1\n"
     ]
    }
   ],
   "source": [
    "#Rename the ratings_df columns, since the build_utility() function requires these column names: \n",
    "#[\"user_id\",\"item_id\",\"rating\"]\n",
    "\n",
    "ratings_df.columns = [\"user_id\",\"item_id\",\"rating\"]\n",
    "\n",
    "#Initialize the lambda1 values\n",
    "#The same values will be used for lambda2 also\n",
    "lambda1 = [0.1,1,2,3,4,5,10,20,30,40,100]\n",
    "\n",
    "#Select the ratings data based on the \n",
    "#following number of users\n",
    "total_users = [59132]\n",
    "\n",
    "#lambda1 = [1,2]\n",
    "#total_users = [20,30]\n",
    "\n",
    "#Learning rate for SGD\n",
    "alpha = 0.00001\n",
    "#alpha = [0.01, 0.001, 0.0001, 0.00001, 0.0000001,0.000000001]\n",
    "\n",
    "#Error tolerance\n",
    "#Maximum error acceptable to terminate the iterations\n",
    "error_diff = 0.0000000001\n",
    "\n",
    "#Seed to reproduce the results\n",
    "seed = 10\n",
    "\n",
    "#Place holders for metrics\n",
    "reg = []\n",
    "num_of_users = []\n",
    "latent_factors = []\n",
    "train_error = []\n",
    "test_error = []\n",
    "run_time = []\n",
    "algorithm = []\n",
    "iterations = []\n",
    "LR = []\n",
    "#Maximum number of iterations\n",
    "n = 1000\n",
    "from time import time\n",
    "\n",
    "df = ratings_df.copy()\n",
    "Utility = build_utility_matrix(df)\n",
    "train_df,test_df=split_data(Utility,test_perc=20)\n",
    "epoch = list()\n",
    "for episode in [1,2,3,4,5]:\n",
    "    print \"Training episode: {}\".format(episode)\n",
    "    train_normalized,train_items_mean,train_users_mean = normalize(train_df)\n",
    "    train_users=train_df.shape[0]\n",
    "    for a in lambda1:\n",
    "        #print \"evaluating lambda={} iteration\".format(a)\n",
    "        c = [2,5,10,15,20,25,30,35,40,50,60,80,100]\n",
    "        for d in c:\n",
    "            #Running SGD for the given parameters combination\n",
    "            #for rate in alpha:\n",
    "                #LR.append(rate)\n",
    "                algorithm.append(\"SGD\")\n",
    "                reg.append(a)\n",
    "                num_of_users.append(b)\n",
    "                latent_factors.append(d)\n",
    "                start = time() # Get start time\n",
    "                U, V, error = SGD_factorization(np.array(train_normalized),d, a, a, n, error_diff, seed,alpha)\n",
    "                end = time() # Get start time\n",
    "                train_error.append(error[-1])\n",
    "                iterations.append(len(error))\n",
    "                run_time.append(end-start)\n",
    "\n",
    "                temp_UV = np.dot(U,V) + train_items_mean\n",
    "                temp_UV = temp_UV.T + train_users_mean\n",
    "                temp_UV = temp_UV.T\n",
    "                predicted_ratings = get_ratings(pd.DataFrame(temp_UV),list(test_df[\"row_number\"]),\n",
    "                                                   list(test_df[\"column_number\"]),indices=False)\n",
    "                test_error.append(np.sqrt((np.nanmean(np.square(\n",
    "                                    np.array(test_df[\"rating\"]) - np.array(predicted_ratings))))))\n",
    "                epoch.append(episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance metrics obtained for various parameters combinations (for all the 5 training episodes) are written to a file, so that we do not have to re-execute the above code block again. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average performance metrics obtained for various parameters combinations.\n",
      "The metrics are sorted in the descending order of average test error\n",
      "Displaying the top rows only.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Reg</th>\n",
       "      <th>Latent_Factors</th>\n",
       "      <th>Avg_Run_Time</th>\n",
       "      <th>Avg_Training_Error</th>\n",
       "      <th>Avg_Test_Error</th>\n",
       "      <th>Avg_Iterations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SGD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "      <td>3.2880</td>\n",
       "      <td>1.890782</td>\n",
       "      <td>3.695142</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>SGD</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60</td>\n",
       "      <td>6.5704</td>\n",
       "      <td>1.890789</td>\n",
       "      <td>3.695788</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>SGD</td>\n",
       "      <td>40.0</td>\n",
       "      <td>60</td>\n",
       "      <td>4.5240</td>\n",
       "      <td>1.890779</td>\n",
       "      <td>3.695791</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>SGD</td>\n",
       "      <td>30.0</td>\n",
       "      <td>60</td>\n",
       "      <td>4.4480</td>\n",
       "      <td>1.890782</td>\n",
       "      <td>3.695791</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>SGD</td>\n",
       "      <td>20.0</td>\n",
       "      <td>60</td>\n",
       "      <td>6.2584</td>\n",
       "      <td>1.890778</td>\n",
       "      <td>3.695793</td>\n",
       "      <td>21.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>SGD</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60</td>\n",
       "      <td>4.4800</td>\n",
       "      <td>1.890769</td>\n",
       "      <td>3.695793</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>SGD</td>\n",
       "      <td>2.0</td>\n",
       "      <td>60</td>\n",
       "      <td>4.6008</td>\n",
       "      <td>1.890782</td>\n",
       "      <td>3.695793</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.1</td>\n",
       "      <td>60</td>\n",
       "      <td>3.0220</td>\n",
       "      <td>1.890780</td>\n",
       "      <td>3.695793</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>SGD</td>\n",
       "      <td>5.0</td>\n",
       "      <td>60</td>\n",
       "      <td>4.6580</td>\n",
       "      <td>1.890803</td>\n",
       "      <td>3.695793</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>SGD</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60</td>\n",
       "      <td>3.1560</td>\n",
       "      <td>1.890777</td>\n",
       "      <td>3.695793</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Algorithm    Reg  Latent_Factors  Avg_Run_Time  Avg_Training_Error  \\\n",
       "23        SGD    1.0              60        3.2880            1.890782   \n",
       "140       SGD  100.0              60        6.5704            1.890789   \n",
       "127       SGD   40.0              60        4.5240            1.890779   \n",
       "114       SGD   30.0              60        4.4480            1.890782   \n",
       "101       SGD   20.0              60        6.2584            1.890778   \n",
       "49        SGD    3.0              60        4.4800            1.890769   \n",
       "36        SGD    2.0              60        4.6008            1.890782   \n",
       "10        SGD    0.1              60        3.0220            1.890780   \n",
       "75        SGD    5.0              60        4.6580            1.890803   \n",
       "62        SGD    4.0              60        3.1560            1.890777   \n",
       "\n",
       "     Avg_Test_Error  Avg_Iterations  \n",
       "23         3.695142             7.0  \n",
       "140        3.695788            23.0  \n",
       "127        3.695791            13.2  \n",
       "114        3.695791            12.6  \n",
       "101        3.695793            21.2  \n",
       "49         3.695793            13.0  \n",
       "36         3.695793            13.4  \n",
       "10         3.695793             5.8  \n",
       "75         3.695793            13.2  \n",
       "62         3.695793             6.2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the performance metrics sorted in the descending order of training error:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Reg</th>\n",
       "      <th>Latent_Factors</th>\n",
       "      <th>Avg_Run_Time</th>\n",
       "      <th>Avg_Training_Error</th>\n",
       "      <th>Avg_Test_Error</th>\n",
       "      <th>Avg_Iterations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>SGD</td>\n",
       "      <td>40.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.9224</td>\n",
       "      <td>1.009886</td>\n",
       "      <td>3.739950</td>\n",
       "      <td>6.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>SGD</td>\n",
       "      <td>4.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.9700</td>\n",
       "      <td>1.009887</td>\n",
       "      <td>3.739955</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SGD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.2340</td>\n",
       "      <td>1.009889</td>\n",
       "      <td>3.739955</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>SGD</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100</td>\n",
       "      <td>3.5940</td>\n",
       "      <td>1.009889</td>\n",
       "      <td>3.739955</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>SGD</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100</td>\n",
       "      <td>3.8440</td>\n",
       "      <td>1.009889</td>\n",
       "      <td>3.739955</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Algorithm   Reg  Latent_Factors  Avg_Run_Time  Avg_Training_Error  \\\n",
       "129       SGD  40.0             100        2.9224            1.009886   \n",
       "64        SGD   4.0             100        2.9700            1.009887   \n",
       "25        SGD   1.0             100        2.2340            1.009889   \n",
       "77        SGD   5.0             100        3.5940            1.009889   \n",
       "51        SGD   3.0             100        3.8440            1.009889   \n",
       "\n",
       "     Avg_Test_Error  Avg_Iterations  \n",
       "129        3.739950             6.4  \n",
       "64         3.739955             6.2  \n",
       "25         3.739955             3.4  \n",
       "77         3.739955             9.6  \n",
       "51         3.739955            10.4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#performance_df = pd.DataFrame(zip(algorithm,reg,num_of_users,latent_factors,run_time,\n",
    "#                                  train_error,test_error,iterations,epoch),\n",
    "#                             columns=['algorithm','reg','num_of_users','latent_factors',\n",
    "#                                      'run_time','train_error','test_error','iterations','epoch'])\n",
    "\n",
    "##Writing the performance metrics to a file, so that we do not have to\n",
    "##run the above block again\n",
    "#performance_df.to_csv(\"performance_df.csv\")\n",
    "performance_df = pd.read_csv(\"performance_df.csv\")\n",
    "#display(performance_df.sort([\"test_error\"]))\n",
    "\n",
    "display_df = performance_df.groupby(['algorithm',\n",
    "                                     'reg','num_of_users',\n",
    "                                     'latent_factors']).mean().reset_index().sort(\"test_error\")[['algorithm',\n",
    "                                                                                                 'reg',\n",
    "                                                                                                 \n",
    "                                                                                                 'latent_factors',\n",
    "                                      'run_time','train_error','test_error','iterations']]\n",
    "display_df.columns = [\"Algorithm\",\"Reg\",\"Latent_Factors\",\"Avg_Run_Time\",\n",
    "                      \"Avg_Training_Error\",\"Avg_Test_Error\",\"Avg_Iterations\"]\n",
    "print \"Average performance metrics obtained for various parameters combinations.\"\n",
    "print \"The metrics are sorted in the descending order of average test error\"\n",
    "print \"Displaying the top rows only.\"\n",
    "display(display_df.head(10))\n",
    "\n",
    "\n",
    "print \"Displaying the performance metrics sorted in the descending order of training error:\"\n",
    "display(display_df.sort([\"Avg_Training_Error\"]).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above display, we can infer the following:\n",
    "* The optimal number of latent factors is 60, since the average test error is the least for 60 latent factors.\n",
    "* The Regularization (Reg) parameter has no effect on the average test error, since the test error is more or less the same for all the regularization parameters, and with 60 latent factors.\n",
    "* The average training error for 100 latent factors is minimum, but the average test error is not the minimum for 100 latent factors. This suggests that overfitting is happening at 100 latent factors\n",
    "\n",
    "Since Reg value has no significant effect on the test error, we will choose the following parameters combinations:\n",
    "\n",
    "$$Reg = 1$$\n",
    "$$\\mbox{Latent Factors} = 60$$\n",
    "$$\\mbox{Learning rate} =  0.00001$$\n",
    "\n",
    "Note that we did NOT vary the learning rate while gathering the performance metrics. Just to make sure how the learning rate effects the test error, the following code block will use the [0.01, 0.001, 0.0001, 0.00001, 0.0000001,0.000000001] values as the learning rate, along the with the above parameter values.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating LR = 0.01\n",
      "Evaluating LR = 0.001\n",
      "Evaluating LR = 0.0001\n",
      "Evaluating LR = 1e-05\n",
      "Evaluating LR = 1e-07\n",
      "Evaluating LR = 1e-09\n",
      "Test Error for various learning rates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Test Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>378.789936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>4.000926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>4.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>4.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>4.000901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>4.000901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Learning Rate  Test Error\n",
       "0   1.000000e-02  378.789936\n",
       "1   1.000000e-03    4.000926\n",
       "2   1.000000e-04    4.000901\n",
       "3   1.000000e-05    4.000901\n",
       "4   1.000000e-07    4.000901\n",
       "5   1.000000e-09    4.000901"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_error_1 = list()\n",
    "test_error_1 = list()\n",
    "iterations_1 = list()\n",
    "ratings_df.columns = [\"user_id\",\"item_id\",\"rating\"]\n",
    "df = ratings_df.copy()\n",
    "Utility = build_utility_matrix(df)\n",
    "train_df,test_df=split_data(Utility,test_perc=20)\n",
    "#train_users_1=train_df.shape[0]\n",
    "train_normalized,train_items_mean,train_users_mean = normalize(train_df)\n",
    "\n",
    "#print train_users\n",
    "\n",
    "#SGD_factorization(np.array(train_normalized),d, a, a, n, error_diff, seed,alpha)\n",
    "alpha_1 = [0.01, 0.001, 0.0001, 0.00001, 0.0000001,0.000000001]\n",
    "\n",
    "for rate in alpha_1:\n",
    "    print \"Evaluating LR = {}\".format(rate)\n",
    "    U, V, error = SGD_factorization(np.array(train_normalized),60, 1, 1, 1000, 0.0000000001, 10,rate)\n",
    "    #print error[-1]\n",
    "\n",
    "    train_error_1.append(error[-1])\n",
    "    iterations_1.append(len(error))\n",
    "    temp_UV = np.dot(U,V) + train_items_mean\n",
    "    temp_UV = temp_UV.T + train_users_mean\n",
    "    temp_UV = temp_UV.T\n",
    "\n",
    "\n",
    "    predicted_ratings = get_ratings(pd.DataFrame(temp_UV),list(test_df[\"row_number\"]),\n",
    "                                                   list(test_df[\"column_number\"]),indices=False)\n",
    "    test_error_1.append(np.sqrt(np.nanmean(np.square(np.array(test_df[\"rating\"]) - np.array(predicted_ratings)))))\n",
    "\n",
    "print \"Test Error for various learning rates:\"\n",
    "pd.DataFrame(zip(alpha_1,test_error_1),columns = [\"Learning Rate\", \"Test Error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate of 0.01 is not optimal. But any other learning rate listed in the above display is optimal, although we stick to the learning rate value of 0.00001, which was used to evaluate the algorithm's performance for various parameters combinations. Also we will select the learning rate which is not too slow (since the convergence will take a long time) or too fast, since the overshooting may occur. In the above display, we obtained a huge test error of 378.79, as a result of overshooting of the gradient descent method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the best possible models and evaluating the ROC Area\n",
    "\n",
    "We will use the following parameters to train the SGD model and plot the ROC curves to identify if the SGD model's performance. These are the parameters which were identified based on the average test error in 5 diffrent runs of the SGD algorithm using different sets of training data.\n",
    "\n",
    "$$Reg = 1$$\n",
    "$$\\mbox{Latent Factors} = 60$$\n",
    "$$\\mbox{Learning rate} =  0.00001$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df,test_df=split_data(Utility,test_perc=20)\n",
    "train_normalized,train_items_mean,train_users_mean = normalize(train_df)\n",
    "\n",
    "\n",
    "U, V, error = SGD_factorization(np.array(train_normalized),60, 1, 1, 1000, 0.0000000001, 10,0.00001)\n",
    "#In the above call, we used 80 latent factors, regularization parms as 1, 1\n",
    "#0.0000000001 is the error tolerance (stop the iteration if the consecutive errors difference is less than\n",
    "# or equal to 0.0000000001)\n",
    "#Learning rate is 0.00001\n",
    "\n",
    "pred = np.dot(U,V) + train_items_mean\n",
    "pred = pred.T + train_users_mean\n",
    "pred = pred.T\n",
    "# pred will have the same shape as the utility matrix. \n",
    "# But pred matrix will have an estimate of the missing entries in the Utility matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the test data predictions SGD\n",
    "The following code will get the predicted ratings for the test data using SGD method. Whenever our model predicts a rating of more than 10 or less than -10, we change that rating to 10 and -10 respectively. Also we will normalize the ratings using the following formula:\n",
    "\n",
    "$$x_{new} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "Since our ratings always belong to $[-10,10]$, we can re-write the above formula as:\n",
    "\n",
    "$$x_{new} = \\frac{x - (-10)}{10 - (-10)} = \\frac{x +10}{20}$$\n",
    "\n",
    "The main advantage of this scaling is that all the new ratings will have a value between $[0,1]$. These new values can be interpreted as the probability that a user likes the movie, and this interpretation will help us to plot the ROC curve for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SGD_predicted_ratings = get_ratings(pd.DataFrame(pred),list(test_df[\"row_number\"]),\n",
    "                                               list(test_df[\"column_number\"]),indices=False)\n",
    "\n",
    "SGD_predicted_ratings = np.array(SGD_predicted_ratings)\n",
    "SGD_predicted_ratings[SGD_predicted_ratings > 10] = 10\n",
    "SGD_predicted_ratings[SGD_predicted_ratings < -10] = -10\n",
    "SGD_predicted_prob = (SGD_predicted_ratings+10)/20\n",
    "#print SGD_predicted_prob\n",
    "#print SGD_predicted_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the predicted ratings and probabilities to the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>column_number</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>SGD_predicted_ratings</th>\n",
       "      <th>SGD_predicted_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.844</td>\n",
       "      <td>-6.046616</td>\n",
       "      <td>0.197669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26349</td>\n",
       "      <td>1</td>\n",
       "      <td>28642</td>\n",
       "      <td>7</td>\n",
       "      <td>6.219</td>\n",
       "      <td>-4.103174</td>\n",
       "      <td>0.294841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35563</td>\n",
       "      <td>2</td>\n",
       "      <td>38333</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.594</td>\n",
       "      <td>-0.762650</td>\n",
       "      <td>0.461867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1206</td>\n",
       "      <td>3</td>\n",
       "      <td>1320</td>\n",
       "      <td>13</td>\n",
       "      <td>-8.938</td>\n",
       "      <td>-3.120420</td>\n",
       "      <td>0.343979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44893</td>\n",
       "      <td>4</td>\n",
       "      <td>48201</td>\n",
       "      <td>15</td>\n",
       "      <td>-5.406</td>\n",
       "      <td>2.311937</td>\n",
       "      <td>0.615597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_number  column_number  user_id  item_id  rating  SGD_predicted_ratings  \\\n",
       "0          73              0       83        5  -3.844              -6.046616   \n",
       "1       26349              1    28642        7   6.219              -4.103174   \n",
       "2       35563              2    38333        8  -3.594              -0.762650   \n",
       "3        1206              3     1320       13  -8.938              -3.120420   \n",
       "4       44893              4    48201       15  -5.406               2.311937   \n",
       "\n",
       "   SGD_predicted_prob  \n",
       "0            0.197669  \n",
       "1            0.294841  \n",
       "2            0.461867  \n",
       "3            0.343979  \n",
       "4            0.615597  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"SGD_predicted_ratings\"] = SGD_predicted_ratings\n",
    "test_df[\"SGD_predicted_prob\"] = SGD_predicted_prob\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the ratings as binary prediction problem, we assume that a user likes a joke, if he gives a rating of more than 5, and dislikes the joke, if he rates the joke less than or equal to 5. Based on this assumption, we will add a new column \"actually_liked\" to the test data. This column will have a 1 if the user has given more than 5 rating to a joke, else this column will have 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>column_number</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>SGD_predicted_ratings</th>\n",
       "      <th>SGD_predicted_prob</th>\n",
       "      <th>actually_liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.844</td>\n",
       "      <td>-6.046616</td>\n",
       "      <td>0.197669</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26349</td>\n",
       "      <td>1</td>\n",
       "      <td>28642</td>\n",
       "      <td>7</td>\n",
       "      <td>6.219</td>\n",
       "      <td>-4.103174</td>\n",
       "      <td>0.294841</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35563</td>\n",
       "      <td>2</td>\n",
       "      <td>38333</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.594</td>\n",
       "      <td>-0.762650</td>\n",
       "      <td>0.461867</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1206</td>\n",
       "      <td>3</td>\n",
       "      <td>1320</td>\n",
       "      <td>13</td>\n",
       "      <td>-8.938</td>\n",
       "      <td>-3.120420</td>\n",
       "      <td>0.343979</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44893</td>\n",
       "      <td>4</td>\n",
       "      <td>48201</td>\n",
       "      <td>15</td>\n",
       "      <td>-5.406</td>\n",
       "      <td>2.311937</td>\n",
       "      <td>0.615597</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_number  column_number  user_id  item_id  rating  SGD_predicted_ratings  \\\n",
       "0          73              0       83        5  -3.844              -6.046616   \n",
       "1       26349              1    28642        7   6.219              -4.103174   \n",
       "2       35563              2    38333        8  -3.594              -0.762650   \n",
       "3        1206              3     1320       13  -8.938              -3.120420   \n",
       "4       44893              4    48201       15  -5.406               2.311937   \n",
       "\n",
       "   SGD_predicted_prob  actually_liked  \n",
       "0            0.197669             0.0  \n",
       "1            0.294841             1.0  \n",
       "2            0.461867             0.0  \n",
       "3            0.343979             0.0  \n",
       "4            0.615597             0.0  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actually_liked = test_df[\"rating\"].copy() \n",
    "actually_liked[actually_liked <= 5] = 0\n",
    "actually_liked[actually_liked > 5] = 1\n",
    "test_df[\"actually_liked\"]=actually_liked\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Area Under the Curve:0.87075\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAGoCAYAAAATsnHAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nApVSSqkjXQI9fUFD6mWbWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbb4f898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let us create a function to plot the ROC curves\n",
    "def plot_roc_curves(fpr,tpr,ax,models,colors=[\"red\",\"green\",\"darkorange\",\"green\",\"black\",\"magenta\",\"cyan\"]):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import auc\n",
    "    lw=2 #Line weight\n",
    "    for key in range(len(fpr)):\n",
    "        line1, = ax.plot(fpr[key], tpr[key], linewidth=2,color=colors[key],\n",
    "                 label=models[key]+' ROC (area = %0.2f)' % auc(fpr[key],tpr[key]))\n",
    "    line2,=ax.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax.legend(loc='lower right',fancybox=True, framealpha=0.5)\n",
    "    \n",
    "    ax.set_ylabel('True positive rate')\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_title('ROC Curve(s)')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Dictionary to collect the False Positive Rates at various probabilities thresholds\n",
    "fpr = dict()\n",
    "\n",
    "#Dictionary to collect the True Positive Rates at various probabilities thresholds\n",
    "tpr = dict()\n",
    "\n",
    "#Dictionary to collect the probability thresholds used to compute the TPR and FPR\n",
    "thresholds=dict()\n",
    "\n",
    "#Get the probabilities that the target class=1 (income > 50K) using unoptimized classifier\n",
    "#clf_test_scores=np.array(pd.DataFrame(clf.predict_proba(X_test))[1])\n",
    "SGD_clf_test_scores=np.array(test_df[\"SGD_predicted_prob\"])\n",
    "\n",
    "#Get the FPR, TPR, thresholds used for unoptimized classifier\n",
    "fpr[0], tpr[0], thresholds[0] = roc_curve(test_df[\"actually_liked\"], y_score=SGD_clf_test_scores, pos_label=1)\n",
    "print \"SGD Area Under the Curve:\" + str(auc(fpr[0],tpr[0]))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "#fig.suptitle('Categorical variables bar plots')\n",
    "models=[\"SGD\"]\n",
    "plot_roc_curves(fpr,tpr,ax,models)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the test data, we obtained an ROC of 0.89, which is a very good score. We used a rating threshold of 5, to determine if a user will like a joke. But how do we know that this threshold is the optimal threshold?\n",
    "\n",
    "We will find the ROC using various thresholds ranging from [0, 10] (in the increments of 0.1), for various random selections of the data, and obtain the average ROC. We will select the threshold, for which the average ROC value is the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting episode: 1\n",
      "Completed episode: 1\n",
      "Processing time for episode: 1 is 683.131000042 seconds\n",
      "Starting episode: 2\n",
      "Completed episode: 2\n",
      "Processing time for episode: 2 is 654.195999861 seconds\n",
      "Starting episode: 3\n",
      "Completed episode: 3\n",
      "Processing time for episode: 3 is 689.368999958 seconds\n",
      "Starting episode: 4\n",
      "Completed episode: 4\n",
      "Processing time for episode: 4 is 645.561000109 seconds\n",
      "Starting episode: 5\n",
      "Completed episode: 5\n",
      "Processing time for episode: 5 is 661.556999922 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "threshold = np.arange(4,10.0,0.1)\n",
    "thres = []\n",
    "roc = []\n",
    "epoch = []\n",
    "\n",
    "for episode in [1,2,3,4,5]:\n",
    "#for episode in [1]:\n",
    "    start = time()\n",
    "    print \"Starting episode: {}\".format(episode)\n",
    "    \n",
    "    for t in threshold:\n",
    "        train_df,test_df=split_data(Utility,test_perc=20)\n",
    "        train_normalized,train_items_mean,train_users_mean = normalize(train_df)\n",
    "\n",
    "        U, V, error = SGD_factorization(np.array(train_normalized),60, 1, 1, 1000, 0.0000000001, 10,0.00001)\n",
    "\n",
    "        pred = np.dot(U,V) + train_items_mean\n",
    "        pred = pred.T + train_users_mean\n",
    "        pred = pred.T\n",
    "        SGD_predicted_ratings = get_ratings(pd.DataFrame(pred),list(test_df[\"row_number\"]),\n",
    "                                            list(test_df[\"column_number\"]),indices=False)\n",
    "        if (np.isnan(SGD_predicted_ratings).any()):\n",
    "            print SGD_predicted_ratings\n",
    "            \n",
    "        SGD_predicted_ratings = np.array(SGD_predicted_ratings)\n",
    "        SGD_predicted_ratings[SGD_predicted_ratings > 10] = 10\n",
    "        SGD_predicted_ratings[SGD_predicted_ratings < -10] = -10\n",
    "        SGD_predicted_prob = (SGD_predicted_ratings+10)/20\n",
    "        test_df[\"SGD_predicted_ratings\"] = SGD_predicted_ratings\n",
    "        test_df[\"SGD_predicted_prob\"] = SGD_predicted_prob\n",
    "        actually_liked = test_df[\"rating\"].copy() \n",
    "        #print len(actually_liked[actually_liked > t]) \n",
    "        #print len(actually_liked[actually_liked <= t]) \n",
    "        if ((len(actually_liked[actually_liked > t]) == 0) | (len(actually_liked[actually_liked <= t]) == 0)):\n",
    "                 continue\n",
    "\n",
    "        actually_liked[actually_liked <= t] = 0\n",
    "        actually_liked[actually_liked > t] = 1\n",
    "        test_df[\"actually_liked\"]=actually_liked\n",
    "        #Dictionary to collect the False Positive Rates at various probabilities thresholds\n",
    "        fpr = dict()\n",
    "        #Dictionary to collect the True Positive Rates at various probabilities thresholds\n",
    "        tpr = dict()\n",
    "\n",
    "        #Dictionary to collect the probability thresholds used to compute the TPR and FPR\n",
    "        thresholds=dict()\n",
    "        SGD_clf_test_scores=np.array(test_df[\"SGD_predicted_prob\"])\n",
    "        if (np.isnan(SGD_clf_test_scores).any()):\n",
    "            print \"SGD_clf_test_scores contains NAN: {}\".format(SGD_clf_test_scores)\n",
    "            print test_df\n",
    "        #print \"passed\"\n",
    "        #if (np.isnan(SGD_clf_test_scores).any()):\n",
    "        #    continue\n",
    "        #if (numpy.isnan(test_df[\"actually_liked\"]).any()):\n",
    "        #    print \"SGD_clf_test_scores contains NAN: {}\".format(SGD_clf_test_scores)\n",
    "            \n",
    "        fpr[0], tpr[0], thresholds[0] = roc_curve(test_df[\"actually_liked\"], y_score=SGD_clf_test_scores, pos_label=1)\n",
    "        #print \"SGD Area Under the Curve:\" + str(auc(fpr[0],tpr[0]))\n",
    "\n",
    "        thres.append(t)\n",
    "        roc.append(auc(fpr[0],tpr[0]))\n",
    "        epoch.append(episode)\n",
    "    end = time()\n",
    "    print \"Completed episode: {}\".format(episode)\n",
    "    print \"Processing time for episode: {} is {} seconds\".format(episode,end-start)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>9.8</td>\n",
       "      <td>0.900194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>8.7</td>\n",
       "      <td>0.891332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>9.4</td>\n",
       "      <td>0.883355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6.8</td>\n",
       "      <td>0.868867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.867379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold       auc\n",
       "58        9.8  0.900194\n",
       "47        8.7  0.891332\n",
       "54        9.4  0.883355\n",
       "28        6.8  0.868867\n",
       "23        6.3  0.867379"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_analysis = pd.DataFrame(zip(thres, roc, epoch),columns=[\"threshold\",\"auc\",\"epoch\"])\n",
    "roc_analysis.to_csv(\"roc_analysis.csv\")\n",
    "roc_analysis = roc_analysis[[\"threshold\",\"auc\"]]\n",
    "#roc_analysis.sort([\"auc\"],ascending=False)\n",
    "display_df=roc_analysis.groupby([\"threshold\"]).mean().reset_index().sort([\"auc\"],ascending=False)\n",
    "display_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRMd/JHBdPuMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14f0e748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = display_df[\"threshold\"]\n",
    "y = display_df[\"auc\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "#ax.fmt_ydata = millions\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Avg AUC\")\n",
    "plt.plot(x, y, 'o')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 5.4 threshold, the variance in the AUC is least. As the threshold increases, the variance in the AUC also increases. So we will use the 5.4 as the threshold value to determine if a user likes a joke. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_number</th>\n",
       "      <th>column_number</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>SGD_predicted_ratings</th>\n",
       "      <th>SGD_predicted_prob</th>\n",
       "      <th>actually_liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.844</td>\n",
       "      <td>-6.046616</td>\n",
       "      <td>0.197669</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26349</td>\n",
       "      <td>1</td>\n",
       "      <td>28642</td>\n",
       "      <td>7</td>\n",
       "      <td>6.219</td>\n",
       "      <td>-4.103174</td>\n",
       "      <td>0.294841</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35563</td>\n",
       "      <td>2</td>\n",
       "      <td>38333</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.594</td>\n",
       "      <td>-0.762650</td>\n",
       "      <td>0.461867</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1206</td>\n",
       "      <td>3</td>\n",
       "      <td>1320</td>\n",
       "      <td>13</td>\n",
       "      <td>-8.938</td>\n",
       "      <td>-3.120420</td>\n",
       "      <td>0.343979</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44893</td>\n",
       "      <td>4</td>\n",
       "      <td>48201</td>\n",
       "      <td>15</td>\n",
       "      <td>-5.406</td>\n",
       "      <td>2.311937</td>\n",
       "      <td>0.615597</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51577</td>\n",
       "      <td>5</td>\n",
       "      <td>55616</td>\n",
       "      <td>16</td>\n",
       "      <td>-1.844</td>\n",
       "      <td>0.249251</td>\n",
       "      <td>0.512463</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49091</td>\n",
       "      <td>6</td>\n",
       "      <td>52763</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>0.452082</td>\n",
       "      <td>0.522604</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42933</td>\n",
       "      <td>7</td>\n",
       "      <td>46100</td>\n",
       "      <td>18</td>\n",
       "      <td>3.375</td>\n",
       "      <td>6.654692</td>\n",
       "      <td>0.832735</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>57204</td>\n",
       "      <td>8</td>\n",
       "      <td>61901</td>\n",
       "      <td>19</td>\n",
       "      <td>3.875</td>\n",
       "      <td>4.797257</td>\n",
       "      <td>0.739863</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>177</td>\n",
       "      <td>9</td>\n",
       "      <td>198</td>\n",
       "      <td>20</td>\n",
       "      <td>2.062</td>\n",
       "      <td>0.704070</td>\n",
       "      <td>0.535203</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>889</td>\n",
       "      <td>10</td>\n",
       "      <td>981</td>\n",
       "      <td>21</td>\n",
       "      <td>5.281</td>\n",
       "      <td>4.442643</td>\n",
       "      <td>0.722132</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19144</td>\n",
       "      <td>11</td>\n",
       "      <td>20954</td>\n",
       "      <td>22</td>\n",
       "      <td>-4.500</td>\n",
       "      <td>1.643040</td>\n",
       "      <td>0.582152</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>39626</td>\n",
       "      <td>12</td>\n",
       "      <td>42599</td>\n",
       "      <td>23</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.395456</td>\n",
       "      <td>0.569773</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18539</td>\n",
       "      <td>13</td>\n",
       "      <td>20254</td>\n",
       "      <td>24</td>\n",
       "      <td>-9.875</td>\n",
       "      <td>-1.169155</td>\n",
       "      <td>0.441542</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1228</td>\n",
       "      <td>14</td>\n",
       "      <td>1344</td>\n",
       "      <td>25</td>\n",
       "      <td>6.781</td>\n",
       "      <td>5.898133</td>\n",
       "      <td>0.794907</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15847</td>\n",
       "      <td>15</td>\n",
       "      <td>17398</td>\n",
       "      <td>26</td>\n",
       "      <td>-4.281</td>\n",
       "      <td>0.143921</td>\n",
       "      <td>0.507196</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>380</td>\n",
       "      <td>16</td>\n",
       "      <td>422</td>\n",
       "      <td>27</td>\n",
       "      <td>9.375</td>\n",
       "      <td>8.949129</td>\n",
       "      <td>0.947456</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>57165</td>\n",
       "      <td>17</td>\n",
       "      <td>61858</td>\n",
       "      <td>28</td>\n",
       "      <td>8.406</td>\n",
       "      <td>6.311209</td>\n",
       "      <td>0.815560</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50439</td>\n",
       "      <td>18</td>\n",
       "      <td>54324</td>\n",
       "      <td>29</td>\n",
       "      <td>-2.094</td>\n",
       "      <td>-1.855340</td>\n",
       "      <td>0.407233</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1173</td>\n",
       "      <td>19</td>\n",
       "      <td>1283</td>\n",
       "      <td>30</td>\n",
       "      <td>-7.781</td>\n",
       "      <td>-4.195494</td>\n",
       "      <td>0.290225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>207</td>\n",
       "      <td>20</td>\n",
       "      <td>229</td>\n",
       "      <td>31</td>\n",
       "      <td>-2.844</td>\n",
       "      <td>0.563873</td>\n",
       "      <td>0.528194</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>48166</td>\n",
       "      <td>21</td>\n",
       "      <td>51728</td>\n",
       "      <td>32</td>\n",
       "      <td>6.219</td>\n",
       "      <td>8.175145</td>\n",
       "      <td>0.908757</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3195</td>\n",
       "      <td>22</td>\n",
       "      <td>3509</td>\n",
       "      <td>33</td>\n",
       "      <td>-6.031</td>\n",
       "      <td>-1.165941</td>\n",
       "      <td>0.441703</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18469</td>\n",
       "      <td>23</td>\n",
       "      <td>20176</td>\n",
       "      <td>34</td>\n",
       "      <td>-6.062</td>\n",
       "      <td>1.170145</td>\n",
       "      <td>0.558507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>58744</td>\n",
       "      <td>24</td>\n",
       "      <td>63539</td>\n",
       "      <td>35</td>\n",
       "      <td>-8.125</td>\n",
       "      <td>-1.332331</td>\n",
       "      <td>0.433383</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>40224</td>\n",
       "      <td>25</td>\n",
       "      <td>43231</td>\n",
       "      <td>36</td>\n",
       "      <td>1.062</td>\n",
       "      <td>4.434813</td>\n",
       "      <td>0.721741</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>49863</td>\n",
       "      <td>26</td>\n",
       "      <td>53643</td>\n",
       "      <td>37</td>\n",
       "      <td>-6.656</td>\n",
       "      <td>0.671231</td>\n",
       "      <td>0.533562</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>37580</td>\n",
       "      <td>27</td>\n",
       "      <td>40444</td>\n",
       "      <td>38</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>0.558808</td>\n",
       "      <td>0.527940</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>55668</td>\n",
       "      <td>28</td>\n",
       "      <td>60222</td>\n",
       "      <td>39</td>\n",
       "      <td>3.188</td>\n",
       "      <td>6.326526</td>\n",
       "      <td>0.816326</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29437</td>\n",
       "      <td>29</td>\n",
       "      <td>31906</td>\n",
       "      <td>40</td>\n",
       "      <td>-1.375</td>\n",
       "      <td>-1.218269</td>\n",
       "      <td>0.439087</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>56315</td>\n",
       "      <td>110</td>\n",
       "      <td>60918</td>\n",
       "      <td>121</td>\n",
       "      <td>-5.156</td>\n",
       "      <td>-0.787222</td>\n",
       "      <td>0.460639</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>32415</td>\n",
       "      <td>111</td>\n",
       "      <td>35022</td>\n",
       "      <td>122</td>\n",
       "      <td>0.594</td>\n",
       "      <td>3.452876</td>\n",
       "      <td>0.672644</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>36295</td>\n",
       "      <td>112</td>\n",
       "      <td>39092</td>\n",
       "      <td>123</td>\n",
       "      <td>4.281</td>\n",
       "      <td>2.309852</td>\n",
       "      <td>0.615493</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>17337</td>\n",
       "      <td>113</td>\n",
       "      <td>18937</td>\n",
       "      <td>124</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-1.489077</td>\n",
       "      <td>0.425546</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>35585</td>\n",
       "      <td>114</td>\n",
       "      <td>38356</td>\n",
       "      <td>125</td>\n",
       "      <td>2.594</td>\n",
       "      <td>4.190505</td>\n",
       "      <td>0.709525</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>49868</td>\n",
       "      <td>115</td>\n",
       "      <td>53649</td>\n",
       "      <td>126</td>\n",
       "      <td>-7.750</td>\n",
       "      <td>-3.750901</td>\n",
       "      <td>0.312455</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>30004</td>\n",
       "      <td>116</td>\n",
       "      <td>32500</td>\n",
       "      <td>127</td>\n",
       "      <td>1.781</td>\n",
       "      <td>5.987565</td>\n",
       "      <td>0.799378</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>13618</td>\n",
       "      <td>117</td>\n",
       "      <td>14909</td>\n",
       "      <td>128</td>\n",
       "      <td>1.875</td>\n",
       "      <td>3.418766</td>\n",
       "      <td>0.670938</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>36966</td>\n",
       "      <td>118</td>\n",
       "      <td>39799</td>\n",
       "      <td>129</td>\n",
       "      <td>8.219</td>\n",
       "      <td>7.606506</td>\n",
       "      <td>0.880325</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>17138</td>\n",
       "      <td>119</td>\n",
       "      <td>18720</td>\n",
       "      <td>130</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>1.132305</td>\n",
       "      <td>0.556615</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>17494</td>\n",
       "      <td>120</td>\n",
       "      <td>19107</td>\n",
       "      <td>131</td>\n",
       "      <td>1.281</td>\n",
       "      <td>0.530585</td>\n",
       "      <td>0.526529</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>54377</td>\n",
       "      <td>121</td>\n",
       "      <td>58796</td>\n",
       "      <td>132</td>\n",
       "      <td>6.906</td>\n",
       "      <td>6.906841</td>\n",
       "      <td>0.845342</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>57435</td>\n",
       "      <td>122</td>\n",
       "      <td>62154</td>\n",
       "      <td>133</td>\n",
       "      <td>0.312</td>\n",
       "      <td>1.843712</td>\n",
       "      <td>0.592186</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>56135</td>\n",
       "      <td>123</td>\n",
       "      <td>60723</td>\n",
       "      <td>134</td>\n",
       "      <td>5.188</td>\n",
       "      <td>6.629437</td>\n",
       "      <td>0.831472</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>30560</td>\n",
       "      <td>124</td>\n",
       "      <td>33089</td>\n",
       "      <td>135</td>\n",
       "      <td>1.625</td>\n",
       "      <td>0.272069</td>\n",
       "      <td>0.513603</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>33998</td>\n",
       "      <td>125</td>\n",
       "      <td>36685</td>\n",
       "      <td>136</td>\n",
       "      <td>4.688</td>\n",
       "      <td>1.688097</td>\n",
       "      <td>0.584405</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>33839</td>\n",
       "      <td>126</td>\n",
       "      <td>36513</td>\n",
       "      <td>137</td>\n",
       "      <td>-1.875</td>\n",
       "      <td>2.980509</td>\n",
       "      <td>0.649025</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>37894</td>\n",
       "      <td>127</td>\n",
       "      <td>40765</td>\n",
       "      <td>138</td>\n",
       "      <td>2.625</td>\n",
       "      <td>3.953124</td>\n",
       "      <td>0.697656</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>45131</td>\n",
       "      <td>128</td>\n",
       "      <td>48446</td>\n",
       "      <td>139</td>\n",
       "      <td>-1.562</td>\n",
       "      <td>2.929601</td>\n",
       "      <td>0.646480</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>37893</td>\n",
       "      <td>129</td>\n",
       "      <td>40764</td>\n",
       "      <td>140</td>\n",
       "      <td>3.188</td>\n",
       "      <td>3.623456</td>\n",
       "      <td>0.681173</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>39555</td>\n",
       "      <td>130</td>\n",
       "      <td>42527</td>\n",
       "      <td>141</td>\n",
       "      <td>-4.156</td>\n",
       "      <td>-1.591921</td>\n",
       "      <td>0.420404</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>16362</td>\n",
       "      <td>131</td>\n",
       "      <td>17920</td>\n",
       "      <td>142</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.630060</td>\n",
       "      <td>0.531503</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>18921</td>\n",
       "      <td>132</td>\n",
       "      <td>20685</td>\n",
       "      <td>143</td>\n",
       "      <td>7.969</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>30130</td>\n",
       "      <td>133</td>\n",
       "      <td>32630</td>\n",
       "      <td>144</td>\n",
       "      <td>-8.250</td>\n",
       "      <td>2.799489</td>\n",
       "      <td>0.639974</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>27025</td>\n",
       "      <td>134</td>\n",
       "      <td>29352</td>\n",
       "      <td>145</td>\n",
       "      <td>-0.719</td>\n",
       "      <td>3.164350</td>\n",
       "      <td>0.658217</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>43421</td>\n",
       "      <td>135</td>\n",
       "      <td>46621</td>\n",
       "      <td>146</td>\n",
       "      <td>1.906</td>\n",
       "      <td>3.728348</td>\n",
       "      <td>0.686417</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>34010</td>\n",
       "      <td>136</td>\n",
       "      <td>36697</td>\n",
       "      <td>147</td>\n",
       "      <td>-4.281</td>\n",
       "      <td>-0.901536</td>\n",
       "      <td>0.454923</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1377</td>\n",
       "      <td>137</td>\n",
       "      <td>1504</td>\n",
       "      <td>148</td>\n",
       "      <td>0.344</td>\n",
       "      <td>2.211108</td>\n",
       "      <td>0.610555</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>18139</td>\n",
       "      <td>138</td>\n",
       "      <td>19807</td>\n",
       "      <td>149</td>\n",
       "      <td>2.312</td>\n",
       "      <td>3.154245</td>\n",
       "      <td>0.657712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>8302</td>\n",
       "      <td>139</td>\n",
       "      <td>9178</td>\n",
       "      <td>150</td>\n",
       "      <td>-2.719</td>\n",
       "      <td>4.462599</td>\n",
       "      <td>0.723130</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     row_number  column_number  user_id  item_id  rating  \\\n",
       "0            73              0       83        5  -3.844   \n",
       "1         26349              1    28642        7   6.219   \n",
       "2         35563              2    38333        8  -3.594   \n",
       "3          1206              3     1320       13  -8.938   \n",
       "4         44893              4    48201       15  -5.406   \n",
       "5         51577              5    55616       16  -1.844   \n",
       "6         49091              6    52763       17  -0.875   \n",
       "7         42933              7    46100       18   3.375   \n",
       "8         57204              8    61901       19   3.875   \n",
       "9           177              9      198       20   2.062   \n",
       "10          889             10      981       21   5.281   \n",
       "11        19144             11    20954       22  -4.500   \n",
       "12        39626             12    42599       23   1.000   \n",
       "13        18539             13    20254       24  -9.875   \n",
       "14         1228             14     1344       25   6.781   \n",
       "15        15847             15    17398       26  -4.281   \n",
       "16          380             16      422       27   9.375   \n",
       "17        57165             17    61858       28   8.406   \n",
       "18        50439             18    54324       29  -2.094   \n",
       "19         1173             19     1283       30  -7.781   \n",
       "20          207             20      229       31  -2.844   \n",
       "21        48166             21    51728       32   6.219   \n",
       "22         3195             22     3509       33  -6.031   \n",
       "23        18469             23    20176       34  -6.062   \n",
       "24        58744             24    63539       35  -8.125   \n",
       "25        40224             25    43231       36   1.062   \n",
       "26        49863             26    53643       37  -6.656   \n",
       "27        37580             27    40444       38  -0.438   \n",
       "28        55668             28    60222       39   3.188   \n",
       "29        29437             29    31906       40  -1.375   \n",
       "..          ...            ...      ...      ...     ...   \n",
       "110       56315            110    60918      121  -5.156   \n",
       "111       32415            111    35022      122   0.594   \n",
       "112       36295            112    39092      123   4.281   \n",
       "113       17337            113    18937      124   0.031   \n",
       "114       35585            114    38356      125   2.594   \n",
       "115       49868            115    53649      126  -7.750   \n",
       "116       30004            116    32500      127   1.781   \n",
       "117       13618            117    14909      128   1.875   \n",
       "118       36966            118    39799      129   8.219   \n",
       "119       17138            119    18720      130  -0.500   \n",
       "120       17494            120    19107      131   1.281   \n",
       "121       54377            121    58796      132   6.906   \n",
       "122       57435            122    62154      133   0.312   \n",
       "123       56135            123    60723      134   5.188   \n",
       "124       30560            124    33089      135   1.625   \n",
       "125       33998            125    36685      136   4.688   \n",
       "126       33839            126    36513      137  -1.875   \n",
       "127       37894            127    40765      138   2.625   \n",
       "128       45131            128    48446      139  -1.562   \n",
       "129       37893            129    40764      140   3.188   \n",
       "130       39555            130    42527      141  -4.156   \n",
       "131       16362            131    17920      142   0.219   \n",
       "132       18921            132    20685      143   7.969   \n",
       "133       30130            133    32630      144  -8.250   \n",
       "134       27025            134    29352      145  -0.719   \n",
       "135       43421            135    46621      146   1.906   \n",
       "136       34010            136    36697      147  -4.281   \n",
       "137        1377            137     1504      148   0.344   \n",
       "138       18139            138    19807      149   2.312   \n",
       "139        8302            139     9178      150  -2.719   \n",
       "\n",
       "     SGD_predicted_ratings  SGD_predicted_prob  actually_liked  \n",
       "0                -6.046616            0.197669             0.0  \n",
       "1                -4.103174            0.294841             1.0  \n",
       "2                -0.762650            0.461867             0.0  \n",
       "3                -3.120420            0.343979             0.0  \n",
       "4                 2.311937            0.615597             0.0  \n",
       "5                 0.249251            0.512463             0.0  \n",
       "6                 0.452082            0.522604             0.0  \n",
       "7                 6.654692            0.832735             0.0  \n",
       "8                 4.797257            0.739863             0.0  \n",
       "9                 0.704070            0.535203             0.0  \n",
       "10                4.442643            0.722132             1.0  \n",
       "11                1.643040            0.582152             0.0  \n",
       "12                1.395456            0.569773             0.0  \n",
       "13               -1.169155            0.441542             0.0  \n",
       "14                5.898133            0.794907             1.0  \n",
       "15                0.143921            0.507196             0.0  \n",
       "16                8.949129            0.947456             1.0  \n",
       "17                6.311209            0.815560             1.0  \n",
       "18               -1.855340            0.407233             0.0  \n",
       "19               -4.195494            0.290225             0.0  \n",
       "20                0.563873            0.528194             0.0  \n",
       "21                8.175145            0.908757             1.0  \n",
       "22               -1.165941            0.441703             0.0  \n",
       "23                1.170145            0.558507             0.0  \n",
       "24               -1.332331            0.433383             0.0  \n",
       "25                4.434813            0.721741             0.0  \n",
       "26                0.671231            0.533562             0.0  \n",
       "27                0.558808            0.527940             0.0  \n",
       "28                6.326526            0.816326             0.0  \n",
       "29               -1.218269            0.439087             0.0  \n",
       "..                     ...                 ...             ...  \n",
       "110              -0.787222            0.460639             0.0  \n",
       "111               3.452876            0.672644             0.0  \n",
       "112               2.309852            0.615493             0.0  \n",
       "113              -1.489077            0.425546             0.0  \n",
       "114               4.190505            0.709525             0.0  \n",
       "115              -3.750901            0.312455             0.0  \n",
       "116               5.987565            0.799378             0.0  \n",
       "117               3.418766            0.670938             0.0  \n",
       "118               7.606506            0.880325             1.0  \n",
       "119               1.132305            0.556615             0.0  \n",
       "120               0.530585            0.526529             0.0  \n",
       "121               6.906841            0.845342             1.0  \n",
       "122               1.843712            0.592186             0.0  \n",
       "123               6.629437            0.831472             1.0  \n",
       "124               0.272069            0.513603             0.0  \n",
       "125               1.688097            0.584405             0.0  \n",
       "126               2.980509            0.649025             0.0  \n",
       "127               3.953124            0.697656             0.0  \n",
       "128               2.929601            0.646480             0.0  \n",
       "129               3.623456            0.681173             0.0  \n",
       "130              -1.591921            0.420404             0.0  \n",
       "131               0.630060            0.531503             0.0  \n",
       "132              10.000000            1.000000             1.0  \n",
       "133               2.799489            0.639974             0.0  \n",
       "134               3.164350            0.658217             0.0  \n",
       "135               3.728348            0.686417             0.0  \n",
       "136              -0.901536            0.454923             0.0  \n",
       "137               2.211108            0.610555             0.0  \n",
       "138               3.154245            0.657712             0.0  \n",
       "139               4.462599            0.723130             0.0  \n",
       "\n",
       "[140 rows x 8 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df, test_df = split_data(Utility,test_perc=20)\n",
    "train_normalized,train_items_mean,train_users_mean = normalize(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0095472469335254, 1.0095508996305076, 1.0095553240065605, 1.0095553240068451]\n",
      "[4.1336934253625524]\n"
     ]
    }
   ],
   "source": [
    "train_error = list()\n",
    "test_error = list()\n",
    "iterations = list()\n",
    "train_users=train_df.shape[0]\n",
    "#print train_users\n",
    "\n",
    "#SGD_factorization(np.array(train_normalized),d, a, a, n, error_diff, seed,alpha)\n",
    "#U, V, error = SGD_factorization(np.array(train_normalized),20, 4, 4, 1000, 0.0000000000001, 1,0.00000001)\n",
    "#3.16\n",
    "#U, V, error = SGD_factorization(np.array(train_normalized),30, .100, .100, 1000, 0.0000000000001, 1,0.00000001)\n",
    "#3.084\n",
    "#U, V, error = SGD_factorization(np.array(train_normalized),40, .100, .100, 1000, 0.0000000000001, 1,0.00000001)\n",
    "#2.98\n",
    "#U, V, error = SGD_factorization(np.array(train_normalized),35, .1, .1, 1000, 0.0000000000001, 1,0.000000001)\n",
    "#2.94\n",
    "U, V, error = SGD_factorization(np.array(train_normalized),100, 100, 100, 1000, 0.0000000001, 10,0.00001)\n",
    "print error\n",
    "\n",
    "train_error.append(error[-1])\n",
    "iterations.append(len(error))\n",
    "#run_time.append(end-start)\n",
    "            \n",
    "temp_UV = np.dot(U,V) + train_items_mean\n",
    "temp_UV = temp_UV.T + train_users_mean\n",
    "temp_UV = temp_UV.T\n",
    "           \n",
    "            \n",
    "predicted_ratings = get_ratings(pd.DataFrame(temp_UV),list(test_df[\"row_number\"]),\n",
    "                                               list(test_df[\"column_number\"]),indices=False)\n",
    "test_error.append(np.sqrt(np.nanmean(np.square(np.array(test_df[\"rating\"]) - np.array(predicted_ratings)))))\n",
    "print test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.0982170371635194]"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.3792170228610461, -1.0236729701955283, -0.92199375258300986, -3.9575798648620499, -1.4934092002816535, 3.6745672776287979, 10.244062374742954, 0.026536082365111055, 2.5842895746042309, 0.91741488199602295, 7.0122958204275712, 3.4010763951279577, -4.8685992171642454, -1.2996424794225148, 0.10670231886971959, 1.760215591465853, 3.6752984165716187, 7.8984295019291881, 1.5973678833967668, 1.9282705390879997, -1.7458750375987768, 4.529654915220565, 1.9434833724237628, 3.5414845123888523, 4.5366243312012244, 1.7269775027600174, -0.74617344942029551, 1.3412365886809772, 4.8509343512675658, -2.5844443680001472, 1.547691537611787, -4.4550812269465112, 0.49911566421949316, 7.2159348556901897, 6.1234676573757145, 1.0840083983769149, 1.018585811218786, 2.9128615733706154, 2.1553240470884329, 7.206377705932983, 2.2776282300021755, -5.1282442914058741, 1.2399471911210087, 2.8501718640287823, 2.6332216438152058, 0.47344685153761157, 5.5967315538092386, -3.5737205813463171, 3.1501471638305554, 1.3522479295512511, 5.4601239671624473, 4.4828796139457836, 2.0423539707935698, -4.1921973274796045, 6.7072190761072923, 1.3426669041379244, -1.7784575713991639, 1.93594256963486, 3.5404740704952342, 4.6550617513147925, -1.7326459970775197, 4.0176023673950727, 2.1694698649528004, -0.5625792764207368, -1.4501809744307605, 4.8987926066817522, -0.16484799372053538, 3.6826655091486113, 2.22071156627797, 3.527234910873565, 5.7895630256976354, -0.48744940342458998, 8.1703001173844978, 1.7523927948617075, -2.1443121160423879, -0.80268384362355905, -0.15924400153452112, -1.1255621031433534, 1.1975611841581686, 0.25047520591951011, 3.9397268098454683, 4.4775735314080478, 3.2663762355569141, 0.59640358795880388, 3.0387127131097889, 0.55093655278592402, 2.1823940329706644, 3.1596402842565974, 2.2458973054963667, 1.0061204706214641, 0.10086740952194992, 7.5549508192181083, 4.9594152287492888, 5.1177613854731829, 3.7396175230178983, 2.8534413194028811, 3.5444614610259908, 3.6136235060343971, 1.4077356880461704, 2.3442636979681715, 2.4197434838561547, 4.1784934207866202, 6.702346230047632, 3.6684764440680264, 5.4710254197949144, 1.6378805302123853, 3.307544080894528, -1.2638532258413575, 3.7323267031048504, -4.4911972868604222, -0.27787451532902185, 6.5854282567276048, -0.67153381208955154, 0.85027827624330898, -2.2890503941826359, 5.681051421845865, 2.3695700684399146, 1.3829534415726648, 3.5815717701252865, 0.60411111447188659, 4.4652556623897217, 0.22439072959225204, -2.0778487480067049, 8.0241327756291216, 1.9220781470556862, 1.0410017212912084, 1.598174982641539, 1.7712307594783925, 1.3495651322889846, -0.99709829728236288, -1.3623211769099377, 6.0362165042936198, 1.853357271422978, 1.1946730759323307, 6.1296321383800052, 3.3576817324573209, 5.3014020214829864, 4.9048201091215615, -1.8156356716431445, -1.0967392686095647]\n",
      "     row_number  column_number  user_id  item_id  rating\n",
      "0           306              0      342        5   4.375\n",
      "1         31982              1    34568        7   3.281\n",
      "2         22715              2    24766        8   3.375\n",
      "3         31140              3    33685       13   1.812\n",
      "4         28407              4    30807       15  -2.969\n",
      "5         17773              5    19411       16   9.469\n",
      "6          8981              6     9917       17   6.844\n",
      "7         58737              7    63532       18   6.219\n",
      "8          2922              8     3176       19   3.250\n",
      "9            83              9       95       20  -1.156\n",
      "10        14562             10    15962       21   7.156\n",
      "11        29802             11    32287       22   7.031\n",
      "12        33150             12    35795       23  -3.688\n",
      "13        30732             13    33265       24   3.094\n",
      "14        13530             14    14804       25   2.531\n",
      "15         1963             15     2127       26  -5.688\n",
      "16          622             16      678       27  -5.156\n",
      "17        13112             17    14336       28   9.031\n",
      "18        15911             18    17463       29   4.094\n",
      "19        37746             19    40614       30   3.500\n",
      "20            5             20        6       31   5.969\n",
      "21        45702             21    49061       32   4.031\n",
      "22         8247             22     9118       33   4.438\n",
      "23        12626             23    13816       34   7.812\n",
      "24        19759             24    21592       35   3.938\n",
      "25        31148             25    33694       36   6.250\n",
      "26        49710             26    53462       37   0.312\n",
      "27        40194             27    43199       38  -5.000\n",
      "28        41237             28    44301       39  -0.469\n",
      "29         7574             29     8366       40   4.281\n",
      "..          ...            ...      ...      ...     ...\n",
      "110       16087            110    17642      121   1.812\n",
      "111       53541            111    57843      122  -4.812\n",
      "112       18481            112    20188      123   3.250\n",
      "113       28644            113    31062      124   5.312\n",
      "114       33381            114    36033      125  -2.250\n",
      "115       57972            115    62715      126   7.969\n",
      "116       23732            116    25861      127   7.375\n",
      "117       42878            117    46040      128   3.344\n",
      "118       43993            118    47231      129  -0.281\n",
      "119       10930            119    12002      130   1.562\n",
      "120       37852            120    40722      131   7.250\n",
      "121       33538            121    36200      132   1.094\n",
      "122       57173            122    61866      133   0.938\n",
      "123         710            123      786      134   8.906\n",
      "124        6033            124     6647      135   9.031\n",
      "125        1595            125     1733      136  -4.625\n",
      "126       24758            126    26942      137  -5.000\n",
      "127       58235            127    62994      138   3.219\n",
      "128       47158            128    50618      139   3.812\n",
      "129       33501            129    36157      140   5.625\n",
      "130       42132            130    45241      141  -0.625\n",
      "131        1833            131     1987      142  -7.844\n",
      "132       32544            132    35165      143   1.812\n",
      "133       43820            133    47040      144   0.938\n",
      "134       30268            134    32775      145   4.375\n",
      "135       30854            135    33394      146  -0.031\n",
      "136       43525            136    46732      147   9.906\n",
      "137       13891            137    15216      148   4.938\n",
      "138       57413            138    62132      149  -5.875\n",
      "139       18625            139    20349      150  -5.438\n",
      "\n",
      "[140 rows x 5 columns]\n",
      "[[ 0.49130807 -9.29325027 -9.29179616 ...,  3.28922179  2.62704287\n",
      "   3.03484055]\n",
      " [-9.54823416  9.88555803  9.4819027  ...,  4.94958281  4.29062623\n",
      "   4.69853354]\n",
      " [-9.83939841 -9.84332396 -7.21830219 ..., -3.98135927 -4.64252141\n",
      "  -4.23182452]\n",
      " ..., \n",
      " [-3.50247717 -7.25003776  4.56195399 ...,  1.32481963  0.6637292\n",
      "   1.07438218]\n",
      " [-3.02163887 -8.53115694 -8.43815365 ...,  1.80608561  1.14502657\n",
      "   1.55566925]\n",
      " [ 0.76459854 -7.90315824 -7.59096647 ...,  8.78430827  8.78443327\n",
      "   7.56559959]]\n"
     ]
    }
   ],
   "source": [
    "print predicted_ratings\n",
    "print test_df\n",
    "print temp_UV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we successfully run the above script in Hadoop environment, I would like to work on the following requirements:\n",
    "\n",
    "\n",
    "## Other requirements\n",
    "\n",
    "1. Determine if SGD algorithm implemented in project-4 (https://github.com/msekhar12/Recommendation_Systems/tree/master/Project_4) can be used on the 1.7Million ratings data set? This will be run in a non-hadoop environment, since SGD is not available in Spark MLLib yet.\n",
    "\n",
    "2. If SGD can be run on the 1.7 Million ratings data, determine the area under the ROC, to quantify the predictive performance of the SGD algorithm, and identify the optimal rating, which can be used to classify whether a user has really liked a joke or not.\n",
    "\n",
    "3. _If I have time_, I would also like to work on building the recommendation system based on the textual data of the jokes text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "1. Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001\n",
    "\n",
    "2. Jure Leskovec, Anand Rajaraman and Jeffrey D. Ullman 2014. Mining of Massive Datasets (Chapter 9)\n",
    "\n",
    "3. Deepak K. Agarwal and Bee-Chung Chen. Statistical Methods for Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix - A\n",
    "Some of the useful hadoop commands are given below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix - B\n",
    "\n",
    "The implementation of the cosine similarity computation process is explained using a small data set. The same logic is applicable to the massive data sets.\n",
    "\n",
    "## Example:\n",
    "\n",
    "Let us assume that we have the following ratings for some of the jokes:\n",
    "\n",
    "<img src=\"toy-example.png\">\n",
    "\n",
    "The above figure shows the normalized ratings. These normalized ratings will be converted to an RDD with user ID as the key (the RDD is displayed below):\n",
    "\n",
    "<img src=\"RDD-1.png\">\n",
    "\n",
    "Let us consider the first element highlighted in red color. This element specifies that the user ID 1 has given a normalized rating of -2.2 to joke 1. The second element highlighted in blue color specifies that the user ID 1 has given a normalized rating of -5.7 to joke 4.\n",
    "\n",
    "This RDD is self joined with itself (on the join condition of USER-ID = USER-ID). The resultant RDD is filtered further to avoid duplicate combinations. For example, for the user ID 4, we will get the following elements (not all permutations are shown):\n",
    "\n",
    "<img src=\"RDD-2a.png\">\n",
    "\n",
    "Consider the second element of the above list. (4, ((1, -6.5), (1, -6.5))), which is highlighted in red. This element is showing the combination of the same joke ID 1. Such elements can be eliminated. Consider third and 4th elements: (4, ((2, -5.0),(1, -6.5))),\n",
    "(4, ((1, -6.5), (2, -5.0))), which are highlighted in green and blue respectively. Both represent the same combinations. We only need one element from such duplicate combinations. So we will filter the elements and include only the elements if and only if the joke ID of the first value is less than the joke ID of the second value. This filter condition will eliminate the second and third rows. When the same condition is applied on the whole data set, we will get the following elements:\n",
    "\n",
    "<img src=\"RDD-2.png\">\n",
    "\n",
    "The above RDD is further reduced, by extracting the joke IDs from the values, making the joke IDs as the keys and making the ratings as the values. Note that the user ID is dropped. We will get the following RDD:\n",
    "\n",
    "<img src=\"RDD-3.png\">\n",
    "\n",
    "\n",
    "The above RDD shows the combinations/pairs of jokes, which are rated by at least one user. For example, one user (highlighted in red) has rated the jokes (1,4) pairs as (-2.2, -5.7) and another user (highlighted in blue) has rated the same joke pairs as (-5.25, -2.75). This RDD is further reduced by grouping the keys (joke pairs) and getting the cosine similarity between the values. For example, for the joke pairs (1,4), we have the following ratings provided by two users: (-2.2, -5.7) and (-5.25, -2.75). Getting the cosine similarity between the vectors $[-2.2, -5.25]$ and $[-5.7, -2.75]$ give us 0.7518. This measure is captured along with the number of users who rated both the movies (in this example, we have 2 users who rated 1,4 movies)\n",
    "\n",
    "This filter will give us the following RDD:\n",
    "\n",
    "<img src=\"RDD-4.png\">\n",
    "\n",
    "The above RDD is written to a HDFS file, and this data set will be used to provide recommendations. For instance if a user has liked the joke-6, and he has not seen the joke-1, then we can recommend joke-1 to him, since the cosine similarity is 0.95.\n",
    "\n",
    "The following code block shows the actual Python code written using PySpark to obtain the cosine similarity of the example data set shown above. This code was written on windows machine with 6 GB RAM, running Spark. The same code(with minor modifications) will be used to run on the bigger data set (with 1.7 Million ratings) on a Hadoop cluster. NOTE that the following code does NOT normalize the ratings automatically. But we will be supplying the data set with normalized ratings as input. But in the implementation of the same program in Hadoop environment, we will also include the logic to normalize the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from math import sqrt\n",
    "import sys\n",
    "\n",
    "## Function to remove duplicate ratings\n",
    "def filterDuplicates((userID, ratings)):\n",
    "    (joke1,rating1) = ratings[0]\n",
    "    (joke2,rating2) = ratings[1]\n",
    "    return joke1 < joke2\n",
    " \n",
    "## Cosine similarity function    \n",
    "def computeCosineSimilarity(ratingPairs):\n",
    "    numPairs = 0\n",
    "    sum_xx = sum_yy = sum_xy = 0\n",
    "    for ratingX, ratingY in ratingPairs:\n",
    "        sum_xx += ratingX * ratingX\n",
    "        sum_yy += ratingY * ratingY\n",
    "        sum_xy += ratingX * ratingY\n",
    "        numPairs += 1\n",
    " \n",
    "    numerator = sum_xy\n",
    "    denominator = sqrt(sum_xx) * sqrt(sum_yy)\n",
    " \n",
    "    score = 0\n",
    "    if (denominator):\n",
    "        score = (numerator / (float(denominator)))\n",
    " \n",
    "    return (score, numPairs)\n",
    " \n",
    "## Make joke pairs as the keys, and their ratings as the values    \n",
    "def makePairs((user, ratings)):\n",
    "    (joke1, rating1) = ratings[0]\n",
    "    (joke2, rating2) = ratings[1]\n",
    "    return ((joke1, joke2), (rating1, rating2))\n",
    " \n",
    "\n",
    "## Define conf object to run on local machine    \n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Ratings..\")\n",
    "\n",
    "## Define SparkContext\n",
    "sc = SparkContext(conf = conf)\n",
    " \n",
    "data = sc.textFile(\"toy_data.csv\")\n",
    "\n",
    "\n",
    "print \"Read the file...\"\n",
    "ratings = data.map(lambda x: x.split(\",\")).map(lambda x: (int(x[0]),(int(x[1]),float(x[2]))))\n",
    "\n",
    "display_df = ratings.collect()\n",
    "\n",
    "for i in display_df:\n",
    "    print i\n",
    "\n",
    "print \"Prepared the ratings RDD...\"\n",
    "\n",
    "## Have to partition the data set when running on a cluster\n",
    "#ratingsPartitioned = ratings.partitionBy(100)\n",
    "#print \"Partitioned the ratings into 100 parts...\"\n",
    "\n",
    "ratingsPartitioned = ratings\n",
    "\n",
    "## Self Join\n",
    "joinedRatings = ratingsPartitioned.join(ratingsPartitioned)\n",
    "print \"Self join completed ...\"\n",
    " \n",
    "##Filter duplicate ratings in the join RDD    \n",
    "uniqueJoinedRatings = joinedRatings.filter(filterDuplicates)\n",
    "print \"Filtered the duplicates...\"\n",
    "\n",
    "display_df = uniqueJoinedRatings.collect()\n",
    "\n",
    "for i in display_df:\n",
    "    print i\n",
    "\n",
    "\n",
    "#jokePairs = uniqueJoinedRatings.map(makePairs).partitionBy(100)\n",
    "jokePairs = uniqueJoinedRatings.map(makePairs)\n",
    "\n",
    "\n",
    "display_df = jokePairs.collect()\n",
    "\n",
    "for i in display_df:\n",
    "    print i\n",
    "\n",
    "jokePairRatings = jokePairs.groupByKey()\n",
    "\n",
    "print \"Computing the cosine similarity ...\"\n",
    "jokePairSimilarities = jokePairRatings.mapValues(computeCosineSimilarity).persist()\n",
    " \n",
    "print \"Sorting the results...\"\n",
    " \n",
    "jokePairSimilarities.sortByKey()\n",
    "\n",
    "display_df = jokePairSimilarities.collect()\n",
    "for i in display_df:\n",
    "    print i\n",
    "\n",
    "print \"Saving the results...\"\n",
    "jokePairSimilarities.saveAsTextFile(\"joke-sims\")\n",
    "\n",
    "print \"script ends...\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix-C\n",
    "\n",
    "## Downloading the files and running the pyspark program\n",
    "\n",
    "FTP the jester_ratings.dat file to a directory in Linux machine, and move the file jester_ratings.dat to the HDFS filesystem, using the following command:\n",
    "\n",
    "hadoop fs -put <local_directory>/jester_ratings.dat  <hadoop_dir> \n",
    "\n",
    "To run the pyspark program in Hadoop, use the following command:\n",
    "\n",
    "       spark-submit --master yarn-client --executor-memory 1g <program_name>\n",
    "\n",
    "The program will create 100 files in HDFS (since we used 100 partitions in our pyspark program). To combine these 100 files and download to the local Linux folder, run the following command:\n",
    "\n",
    "       hadoop fs -getmerge <Hadoop home dir>/joke-sims/ jokes_sim.txt\n",
    "\n",
    "\n",
    "Example:\n",
    "If you have downloaded jester_ratings.dat to /home/sekhar/ directory, you may copy the file to Hadoop File system as follows:\n",
    "\n",
    "1. Create a HDFS directory \"data\" in /user/sekhar directory (which is in HDFS), use the following command: \n",
    "\n",
    "        hadoop fs -mkdir /user/sekhar/data  \n",
    "        \n",
    "2. To copy the downloaded file to /user/sekhar/data HDFS directory, use the following command:\n",
    "        \n",
    "        hadoop fs -put /home/sekhar/jester_ratings.dat  /user/sekhar/data\n",
    "\n",
    "3. To run the pyspark program to find the similarity score, copy the code given in the next block to a file on one of the linux machines in the cluster. For example, if you have copied the code to the file jester_1.py, execute the following command to run:\n",
    "\n",
    "        spark-submit --master yarn-client --executor-memory 1g jester_1.py\n",
    "\n",
    "4. The above command will create 100 HDFS files. These files can be combined into one file and can be downloaded to a local Linux directory:\n",
    "\n",
    "        hadoop fs -getmerge /user/sekhar/joke-sims/ jokes_sim.txt\n",
    "       \n",
    "where /user/sekhar/ is the Hadoop home directory, where the files are created. The joke-sims directory in /user/sekhar/ contains the 100 files.\n",
    "\n",
    "Download the jokes_sim.txt to your local machine, and perform the analysis. This file contains the similarity scores between the jokes.\n",
    "\n",
    "\n",
    "PySpark code to compute the cosine similarity between the pairs of jokes rated by at least one user is given below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "##To run: Copy this program to a file named \"jester_1.py\" and execute the following command:\n",
    "## spark-submit --master yarn-client --executor-memory 1g jester_1.py\n",
    "\n",
    "## Logic:\n",
    "## The following steps will normalize the ratings:\n",
    "## 1. Get the average ratings of all the users given to various items\n",
    "## 2. Get the average ratings of all the items given by various users \n",
    "## 3. Subtract the average ratings of the respective user and the respective item\n",
    "##    from the actual rating given by a user to an item\n",
    "## The following step will compute the cosine similarity between all the pairs of jokes\n",
    "## which are rated by at least one user\n",
    "## 4. Compute the cosine similarity between the pairs of jokes\n",
    "##\n",
    "\n",
    "print \"Starting the script ...\"\n",
    "\n",
    "## filterDuplicates will remove the duplicates \n",
    "def filterDuplicates( (userID, ratings) ):\n",
    "    (joke1, rating1) = ratings[0]\n",
    "    (joke2, rating2) = ratings[1]\n",
    "    return joke1 < joke2\n",
    "\n",
    "## computeCosineSimilarity will compute the cosine similarity\n",
    "def computeCosineSimilarity(ratingPairs):\n",
    "    numPairs = 0\n",
    "    sum_xx = sum_yy = sum_xy = 0\n",
    "    for ratingX, ratingY in ratingPairs:\n",
    "        sum_xx += ratingX * ratingX\n",
    "        sum_yy += ratingY * ratingY\n",
    "        sum_xy += ratingX * ratingY\n",
    "        numPairs += 1\n",
    "\n",
    "    numerator = sum_xy\n",
    "    denominator = sqrt(sum_xx) * sqrt(sum_yy)\n",
    "\n",
    "    score = 0\n",
    "    if (denominator):\n",
    "        score = (numerator / (float(denominator)))\n",
    "\n",
    "    return (score, numPairs)\n",
    "\n",
    "## makes the pairs of jokes as the keys, and their ratings as the values\n",
    "def makePairs((user, ratings)):\n",
    "    (joke1, rating1) = ratings[0]\n",
    "    (joke2, rating2) = ratings[1]\n",
    "    return ((joke1, joke2), (rating1, rating2))\n",
    "\n",
    "## Subtract the users average ratings from the jokes ratings.\n",
    "## The output RDD will have the joke ID as the key\n",
    "def normalizeByUser((user_id,ratings)):\n",
    "    (part_1,avg) = ratings\n",
    "    normalized=part_1[1] - avg\n",
    "    #The key will be the joke ID in the returned RDD\n",
    "    return (part_1[0],(user_id,normalized))\n",
    "## Subtract the jokes average ratings from the respective jokes\n",
    "## The input RDD will be the result of the join between the RDD \n",
    "## obtained by normalizeByUser() function, and the average jokes ratings RDD\n",
    "## The output of this function will be an RDD with user ID as the key\n",
    "def normalizeByJoke((joke_id,ratings)):\n",
    "    (part_1,avg) = ratings\n",
    "    normalized=part_1[1] - avg\n",
    "    #The key will be the user ID in the returned RDD\n",
    "    return (part_1[0],(joke_id,normalized))\n",
    "\n",
    "\n",
    "## Main script\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "data = sc.textFile(\"hdfs:///user/a152700/data/jester_ratings.dat\")\n",
    "\n",
    "##To test on toy.csv data set, uncomment the following statement, and comment the \n",
    "##above statement\n",
    "#data = sc.textFile(\"file:///home/a152700/spark/programs/toy.csv\")\n",
    "print \"Read the file...\"\n",
    "joke_ratings = data.map(lambda x: x.split()).map(lambda x: (int(x[1]),float(x[2])))\n",
    "user_ratings = data.map(lambda x: x.split()).map(lambda x: (int(x[0]),float(x[2])))\n",
    "ratings = data.map(lambda x: x.split()).map(lambda x: (int(x[0]),(int(x[1]),float(x[2]))))\n",
    "\n",
    "print \"Prepared the ratings RDD...\"\n",
    "\n",
    "joke_ratingsPartitioned = joke_ratings.partitionBy(100)\n",
    "print \"Partitioned the joke ratings into 100 parts...\"\n",
    "\n",
    "user_ratingsPartitioned = user_ratings.partitionBy(100)\n",
    "print \"Partitioned the user ratings into 100 parts...\"\n",
    "\n",
    "ratingsPartitioned = ratings.partitionBy(100)\n",
    "print \"Partitioned the whole  ratings into 100 parts...\"\n",
    "\n",
    "##Get the average joke ratings\n",
    "joke_ratings_rdd = joke_ratings.mapValues(lambda x: (x,1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "joke_ratings_avg = joke_ratings_rdd.mapValues(lambda x: x[0]/x[1]).persist()\n",
    "\n",
    "##Get the average user ratings\n",
    "user_ratings_rdd = user_ratings.mapValues(lambda x: (x,1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "user_ratings_avg = user_ratings_rdd.mapValues(lambda x: x[0]/x[1]).persist()\n",
    "\n",
    "#Joining user_ratings_avg and ratings\n",
    "user_ratings_joined = ratingsPartitioned.join(user_ratings_avg)\n",
    "user_ratings_normalized = user_ratings_joined.map(normalizeByUser)\n",
    "#The user_ratings_normalized RDD will have the joke ID as the key\n",
    "#The ratings are normalized using the avg. user rating\n",
    "\n",
    "display_rdd = user_ratings_joined.collect()\n",
    "\n",
    "for i in range(10):\n",
    "     print display_rdd[i]\n",
    "\n",
    "display_rdd = user_ratings_normalized.collect()\n",
    "\n",
    "for i in range(10):\n",
    "     print display_rdd[i]\n",
    "\n",
    "\n",
    "\n",
    "#Joining joke_ratings_avg and ratings\n",
    "joke_ratings_joined = user_ratings_normalized.join(joke_ratings_avg)\n",
    "joke_ratings_normalized = joke_ratings_joined.map(normalizeByJoke) \n",
    "#The joke_ratings_normalized RDD will have the user ID as the key\n",
    "#The ratings are normalized using the avg. joke rating\n",
    "\n",
    "display_rdd = joke_ratings_normalized.collect()\n",
    "\n",
    "for i in range(10):\n",
    "     print display_rdd[i]\n",
    "\n",
    "\n",
    "#joke_ratings_avg_sorted = joke_ratings_avg.sortByKey() \n",
    "\n",
    "#joke_ratings_avg_sorted.saveAsTextFile(\"joke-avg\")\n",
    "\n",
    "#user_ratings_avg_sorted = user_ratings_avg.sortByKey()\n",
    "\n",
    "#user_ratings_avg_sorted.saveAsTextFile(\"user-avg\")\n",
    "\n",
    "\n",
    "\n",
    "#display_rdd = rdd.collect()\n",
    "\n",
    "#for i in range(10):\n",
    "#     print display_rdd[i]\n",
    "\n",
    "\n",
    "##Computing the similarities of jokes using the normalized ratings:\n",
    "##joke_ratings_normalized\n",
    "print \"Normalization of ratings complete...\"\n",
    "print \"Now computing the similarities...\"\n",
    "\n",
    "ratingsPartitioned = joke_ratings_normalized.partitionBy(100)\n",
    "\n",
    "print \"script ends...\"\n",
    "\n",
    "print \"Partitioned the ratings into 100 parts...\"\n",
    "\n",
    "joinedRatings = ratingsPartitioned.join(ratingsPartitioned)\n",
    "print \"Self join completed ...\"\n",
    "\n",
    "uniqueJoinedRatings = joinedRatings.filter(filterDuplicates)\n",
    "print \"Filtered the duplicates...\"\n",
    "\n",
    "jokePairs = uniqueJoinedRatings.map(makePairs).partitionBy(100)\n",
    "\n",
    "jokePairRatings = jokePairs.groupByKey()\n",
    "print \"Computing the cosine similarity ...\"\n",
    "jokePairSimilarities = jokePairRatings.mapValues(computeCosineSimilarity).persist()\n",
    "\n",
    "print \"Sorting the results...\"\n",
    "\n",
    "jokePairSimilarities.sortByKey()\n",
    "\n",
    "print \"Saving the results...\"\n",
    "jokePairSimilarities.saveAsTextFile(\"joke-sims\")\n",
    "display_rdd = jokePairSimilarities.collect()\n",
    "\n",
    "for i in range(10):\n",
    "     print display_rdd[i]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
